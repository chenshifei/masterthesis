%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Shifei Chen at 2020-08-25 10:47:08 +0200 


%% Saved with string encoding Unicode (UTF-8) 

@inproceedings{aharoni-etal-2019-massively,
  title = "Massively Multilingual Neural Machine Translation",
  author = "Aharoni, Roee  and Johnson, Melvin  and Firat, Orhan",
  booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
  month = jun,
  year = "2019",
  address = "Minneapolis, Minnesota",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/N19-1388",
  doi = "10.18653/v1/N19-1388",
  pages = "3874--3884",
  abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}

@inproceedings{littell-etal-2017-uriel,
    title = "{URIEL} and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
    author = "Littell, Patrick  and
      Mortensen, David R.  and
      Lin, Ke  and
      Kairis, Katherine  and
      Turner, Carlisle  and
      Levin, Lori",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2002",
    pages = "8--14",
    abstract = "We introduce the URIEL knowledge base for massively multilingual NLP and the lang2vec utility, which provides information-rich vector identifications of languages drawn from typological, geographical, and phylogenetic databases and normalized to have straightforward and consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors.",
}

@InProceedings{heinzerling2018bpemb,
  author = {Benjamin Heinzerling and Michael Strube},
  title = "{BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages}",
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {May 7-12, 2018},
  address = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {979-10-95546-00-9},
  language = {english}
  }

@article{fasttext,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  year={2017},
  issn={2307-387X},
  pages={135--146}
}

@inproceedings{lu-etal-2018-neural,
  title = "A neural interlingua for multilingual machine translation",
  author = "Lu, Yichao  and Keung, Phillip  and Ladhak, Faisal  and Bhardwaj, Vikas  and Zhang, Shaonan  and Sun, Jason",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  month = oct,
  year = "2018",
  address = "Brussels, Belgium",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W18-6309",
  doi = "10.18653/v1/W18-6309",
  pages = "84--92",
  abstract = "We incorporate an explicit neural interlingua into a multilingual encoder-decoder neural machine translation (NMT) architecture. We demonstrate that our model learns a language-independent representation by performing direct zero-shot translation (without using pivot translation), and by using the source sentence embeddings to create an English Yelp review classifier that, through the mediation of the neural interlingua, can also classify French and German reviews. Furthermore, we show that, despite using a smaller number of parameters than a pairwise collection of bilingual NMT models, our approach produces comparable BLEU scores for each language pair in WMT15.",
}

@article{Arthur:2016aa,
	Abstract = {Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.},
	Author = {Philip Arthur and Graham Neubig and Satoshi Nakamura},
	Date-Added = {2020-08-23 11:01:02 +0200},
	Date-Modified = {2020-08-23 11:01:02 +0200},
	Eprint = {1606.02006},
	Month = {06},
	Title = {Incorporating Discrete Translation Lexicons into Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1606.02006.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA2LjAyMDA2LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA2LjAyMDA2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNi4wMjAwNi5wZGYADgAeAA4AMQA2ADAANgAuADAAMgAwADAANgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDYuMDIwMDYucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.02006}}

@inproceedings{Zoph:2016aa,
    title = "Transfer Learning for Low-Resource Neural Machine Translation",
    author = "Zoph, Barret  and
      Yuret, Deniz  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1163",
    doi = "10.18653/v1/D16-1163",
    pages = "1568--1575",
}

@inproceedings{Nguyen:2017aa,
    title = "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation",
    author = "Nguyen, Toan Q.  and
      Chiang, David",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/I17-2050",
    pages = "296--301",
    abstract = "We present a simple method to improve neural translation of a low-resource language pair using parallel data from a related, also low-resource, language pair. The method is based on the transfer method of Zoph et al., but whereas their method ignores any source vocabulary overlap, ours exploits it. First, we split words using Byte Pair Encoding (BPE) to increase vocabulary overlap. Then, we train a model on the first language pair and transfer its parameters, including its source word embeddings, to another model and continue training on the second language pair. Our experiments show that transfer learning helps word-based translation only slightly, but when used on top of a much stronger BPE baseline, it yields larger improvements of up to 4.3 BLEU.",
}

@article{Koehn:2017aa,
	Abstract = {We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.},
	Author = {Philipp Koehn and Rebecca Knowles},
	Date-Added = {2020-08-22 12:26:48 +0200},
	Date-Modified = {2020-08-22 14:33:44 +0200},
	Eprint = {1706.03872},
	Month = {06},
	Title = {Six Challenges for Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1706.03872.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzA2LjAzODcyLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzA2LjAzODcyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcwNi4wMzg3Mi5wZGYADgAeAA4AMQA3ADAANgAuADAAMwA4ADcAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MDYuMDM4NzIucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1706.03872}}

@article{johnson1984extensions,
	Author = {Johnson, William B and Lindenstrauss, Joram},
	Date-Added = {2020-08-17 14:05:07 +0200},
	Date-Modified = {2020-08-17 14:05:07 +0200},
	Journal = {Contemporary mathematics},
	Number = {189-206},
	Pages = {1},
	Title = {Extensions of Lipschitz mappings into a Hilbert space},
	Volume = {26},
	Year = {1984}}

@article{Maneewongvatana:aa,
	Abstract = {We present an empirical analysis of data structures for approximate nearest neighbor searching. We compare the well-known optimized kd-tree splitting method against two alternative splitting methods. The first, called the sliding-midpoint method, which attempts to balance the goals of producing subdivision cells of bounded aspect ratio, while not producing any empty cells. The second, called the minimum-ambiguity method is a query-based approach. In addition to the data points, it is also given a training set of query points for preprocessing. It employs a simple greedy algorithm to select the splitting plane that minimizes the average amount of ambiguity in the choice of the nearest neighbor for the training points. We provide an empirical analysis comparing these two methods against the optimized kd-tree construction for a number of synthetically generated data and query sets. We demonstrate that for clustered data and query sets, these algorithms can provide significant improvements over the standard kd-tree construction for approximate nearest neighbor searching.},
	Author = {Songrit Maneewongvatana and David M. Mount},
	Date-Added = {2020-08-17 13:16:54 +0200},
	Date-Modified = {2020-08-17 13:16:54 +0200},
	Eprint = {cs/9901013},
	Title = {Analysis of approximate nearest neighbor searching with clustered point sets},
	Url = {https://arxiv.org/pdf/cs/9901013.pdf},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAlLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy85OTAxMDEzLnBkZk8RAUgAAAAAAUgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////ws5OTAxMDEzLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAKy86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6OTkwMTAxMy5wZGYAAA4AGAALADkAOQAwADEAMAAxADMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAClVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy85OTAxMDEzLnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQATAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGY},
	Bdsk-Url-1 = {https://arxiv.org/abs/cs/9901013}}

@article{Virtanen:2019aa,
	Abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
	Author = {Pauli Virtanen and Ralf Gommers and Travis E. Oliphant and Matt Haberland and Tyler Reddy and David Cournapeau and Evgeni Burovski and Pearu Peterson and Warren Weckesser and Jonathan Bright and St{\'e}fan J. van der Walt and Matthew Brett and Joshua Wilson and K. Jarrod Millman and Nikolay Mayorov and Andrew R. J. Nelson and Eric Jones and Robert Kern and Eric Larson and CJ Carey and {\.I}lhan Polat and Yu Feng and Eric W. Moore and Jake VanderPlas and Denis Laxalde},
	Date-Added = {2020-08-17 11:16:45 +0200},
	Date-Modified = {2020-08-17 11:16:45 +0200},
	Doi = {10.1038/s41592-019-0686-2},
	Eprint = {1907.10121},
	Month = {07},
	Title = {SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python},
	Url = {https://arxiv.org/pdf/1907.10121.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTA3LjEwMTIxLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTA3LjEwMTIxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkwNy4xMDEyMS5wZGYADgAeAA4AMQA5ADAANwAuADEAMAAxADIAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MDcuMTAxMjEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1907.10121},
	Bdsk-Url-2 = {https://doi.org/10.1038/s41592-019-0686-2}}

@inproceedings{papineni-etal-2002-bleu,
	Address = {Philadelphia, Pennsylvania, USA},
	Author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	Booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
	Date-Added = {2020-08-10 16:17:37 +0200},
	Date-Modified = {2020-08-10 16:17:37 +0200},
	Doi = {10.3115/1073083.1073135},
	Month = jul,
	Pages = {311--318},
	Publisher = {Association for Computational Linguistics},
	Title = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
	Url = {https://www.aclweb.org/anthology/P02-1040.pdf},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9wMDItMTA0MC5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8McDAyLTEwNDAucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOnAwMi0xMDQwLnBkZgAOABoADABwADAAMgAtADEAMAA0ADAALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9wMDItMTA0MC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P02-1040}}

@inproceedings{Denkowski:2017aa,
    title = "Stronger Baselines for Trustable Results in Neural Machine Translation",
    author = "Denkowski, Michael  and
      Neubig, Graham",
    booktitle = "Proceedings of the First Workshop on Neural Machine Translation",
    month = aug,
    year = "2017",
    address = "Vancouver",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-3203",
    doi = "10.18653/v1/W17-3203",
    pages = "18--27",
    abstract = "Interest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios. New research regularly introduces architectural and algorithmic improvements that lead to significant gains over {``}vanilla{''} NMT implementations. However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-the-art production and shared-task systems. As a result, it is often difficult to determine whether improvements from research will carry over to systems deployed for real-world use. In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems. Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed. We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen. This indicates that choosing a strong baseline is crucial for reporting reliable experimental results.",
}

@inproceedings{Kingma:2014aa,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Bahdanau:2014aa,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Neubig:2018aa,
	Abstract = {This paper describes XNMT, the eXtensible Neural Machine Translation toolkit. XNMT distin- guishes itself from other open-source NMT toolkits by its focus on modular code design, with the purpose of enabling fast iteration in research and replicable, reliable results. In this paper we describe the design of XNMT and its experiment configuration system, and demonstrate its utility on the tasks of machine translation, speech recognition, and multi-tasked machine translation/parsing. XNMT is available open-source at https://github.com/neulab/xnmt},
	Author = {Graham Neubig and Matthias Sperber and Xinyi Wang and Matthieu Felix and Austin Matthews and Sarguna Padmanabhan and Ye Qi and Devendra Singh Sachan and Philip Arthur and Pierre Godard and John Hewitt and Rachid Riad and Liming Wang},
	Date-Added = {2020-08-10 15:45:45 +0200},
	Date-Modified = {2020-08-10 15:45:45 +0200},
	Eprint = {1803.00188},
	Month = {03},
	Title = {XNMT: The eXtensible Neural Machine Translation Toolkit},
	Url = {https://arxiv.org/pdf/1803.00188.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODAzLjAwMTg4LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODAzLjAwMTg4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgwMy4wMDE4OC5wZGYADgAeAA4AMQA4ADAAMwAuADAAMAAxADgAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MDMuMDAxODgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1803.00188}}

@inproceedings{glorot2010understanding,
	Author = {Glorot, Xavier and Bengio, Yoshua},
	Booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	Date-Added = {2020-07-26 01:48:27 +0200},
	Date-Modified = {2020-07-26 01:48:27 +0200},
	Pages = {249--256},
	Title = {Understanding the difficulty of training deep feedforward neural networks},
	Year = {2010}}

@article{Heinzerling:2019aa,
	Abstract = {Pretrained contextual and non-contextual subword embeddings have become available in over 250 languages, allowing massively multilingual NLP. However, while there is no dearth of pretrained embeddings, the distinct lack of systematic evaluations makes it difficult for practitioners to choose between them. In this work, we conduct an extensive evaluation comparing non-contextual subword embeddings, namely FastText and BPEmb, and a contextual representation method, namely BERT, on multilingual named entity recognition and part-of-speech tagging. We find that overall, a combination of BERT, BPEmb, and character representations works best across languages and tasks. A more detailed analysis reveals different strengths and weaknesses: Multilingual BERT performs well in medium- to high-resource languages, but is outperformed by non-contextual subword embeddings in a low-resource setting.},
	Author = {Benjamin Heinzerling and Michael Strube},
	Date-Added = {2020-07-23 02:21:43 +0200},
	Date-Modified = {2020-07-23 02:21:43 +0200},
	Eprint = {1906.01569},
	Month = {06},
	Title = {Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation},
	Url = {https://arxiv.org/pdf/1906.01569.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTA2LjAxNTY5LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTA2LjAxNTY5LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkwNi4wMTU2OS5wZGYADgAeAA4AMQA5ADAANgAuADAAMQA1ADYAOQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MDYuMDE1NjkucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1906.01569}}

@inproceedings{Vaswani:2017aa,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, undefinedukasz and Polosukhin, Illia},
title = {Attention is All You Need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{inproceedings,
	Author = {Di Gangi, Mattia and Federico, Marcello},
	Date-Added = {2020-07-17 01:28:20 +0200},
	Date-Modified = {2020-07-17 01:28:20 +0200},
	Month = {12},
	Title = {Monolingual Embeddings for Low Resourced Neural Machine Translation},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA4Li4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9tb25vbGluZ3VhbC1lbWJlZGRpbmdzLWxvdy5wZGZPEQGSAAAAAAGSAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8ebW9ub2xpbmd1YWwtZW1iZWRkaW5ncy1sb3cucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAD4vOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOm1vbm9saW5ndWFsLWVtYmVkZGluZ3MtbG93LnBkZgAOAD4AHgBtAG8AbgBvAGwAaQBuAGcAdQBhAGwALQBlAG0AYgBlAGQAZABpAG4AZwBzAC0AbABvAHcALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASADxVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9tb25vbGluZ3VhbC1lbWJlZGRpbmdzLWxvdy5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQAXwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAH1},
	Bdsk-Url-1 = {https://www.researchgate.net/publication/321706277_Monolingual_Embeddings_for_Low_Resourced_Neural_Machine_Translation}}

@article{Artetxe:2017aa,
	Abstract = {In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.},
	Author = {Mikel Artetxe and Gorka Labaka and Eneko Agirre and Kyunghyun Cho},
	Date-Added = {2020-07-17 01:24:59 +0200},
	Date-Modified = {2020-07-17 01:24:59 +0200},
	Eprint = {1710.11041},
	Month = {10},
	Title = {Unsupervised Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1710.11041.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzEwLjExMDQxLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzEwLjExMDQxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcxMC4xMTA0MS5wZGYADgAeAA4AMQA3ADEAMAAuADEAMQAwADQAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MTAuMTEwNDEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1710.11041}}

@inproceedings{neishi-etal-2017-bag,
	Abstract = {In this paper, we describe the team UT-IIS{'}s system and results for the WAT 2017 translation tasks. We further investigated several tricks including a novel technique for initializing embedding layers using only the parallel corpus, which increased the BLEU score by 1.28, found a practical large batch size of 256, and gained insights regarding hyperparameter settings. Ultimately, our system obtained a better result than the state-of-the-art system of WAT 2016. Our code is available on \url{https://github.com/nem6ishi/wat17}.},
	Address = {Taipei, Taiwan},
	Author = {Neishi, Masato and Sakuma, Jin and Tohda, Satoshi and Ishiwatari, Shonosuke and Yoshinaga, Naoki and Toyoda, Masashi},
	Booktitle = {Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017)},
	Date-Added = {2020-07-17 01:23:26 +0200},
	Date-Modified = {2020-07-17 01:23:26 +0200},
	Month = nov,
	Pages = {99--109},
	Publisher = {Asian Federation of Natural Language Processing},
	Title = {A Bag of Useful Tricks for Practical Neural Machine Translation: Embedding Layer Initialization and Large Batch Size},
	Url = {https://www.aclweb.org/anthology/W17-5708.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy93MTctNTcwOC5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MdzE3LTU3MDgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOncxNy01NzA4LnBkZgAOABoADAB3ADEANwAtADUANwAwADgALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy93MTctNTcwOC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W17-5708}}

@article{Blackwood:2018aa,
	Abstract = {Multilingual machine translation addresses the task of translating between multiple source and target languages. We propose task-specific attention models, a simple but effective technique for improving the quality of sequence-to-sequence neural multilingual translation. Our approach seeks to retain as much of the parameter sharing generalization of NMT models as possible, while still allowing for language-specific specialization of the attention model to a particular language-pair or task. Our experiments on four languages of the Europarl corpus show that using a target-specific model of attention provides consistent gains in translation quality for all possible translation directions, compared to a model in which all parameters are shared. We observe improved translation quality even in the (extreme) low-resource zero-shot translation directions for which the model never saw explicitly paired parallel data.},
	Author = {Graeme Blackwood and Miguel Ballesteros and Todd Ward},
	Date-Added = {2020-07-17 00:33:21 +0200},
	Date-Modified = {2020-07-17 00:33:21 +0200},
	Eprint = {1806.03280},
	Month = {06},
	Title = {Multilingual Neural Machine Translation with Task-Specific Attention},
	Url = {https://arxiv.org/pdf/1806.03280.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODA2LjAzMjgwLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODA2LjAzMjgwLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgwNi4wMzI4MC5wZGYADgAeAA4AMQA4ADAANgAuADAAMwAyADgAMAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MDYuMDMyODAucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1806.03280}}

@article{Firat:2016aa,
	Abstract = {In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.},
	Author = {Orhan Firat and Baskaran Sankaran and Yaser Al-Onaizan and Fatos T. Yarman Vural and Kyunghyun Cho},
	Date-Added = {2020-07-17 00:32:21 +0200},
	Date-Modified = {2020-07-17 00:32:21 +0200},
	Eprint = {1606.04164},
	Month = {06},
	Title = {Zero-Resource Translation with Multi-Lingual Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1606.04164.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA2LjA0MTY0LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA2LjA0MTY0LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNi4wNDE2NC5wZGYADgAeAA4AMQA2ADAANgAuADAANAAxADYANAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDYuMDQxNjQucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.04164}}

@article{Lakew:2018ab,
	Abstract = {Recently, neural machine translation (NMT) has been extended to multilinguality, that is to handle more than one translation direction with a single system. Multilingual NMT showed competitive performance against pure bilingual systems. Notably, in low-resource settings, it proved to work effectively and efficiently, thanks to shared representation space that is forced across languages and induces a sort of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot inference across language pairs never seen at training time. Despite the increasing interest in this framework, an in-depth analysis of what a multilingual NMT model is capable of and what it is not is still missing. Motivated by this, our work (i) provides a quantitative and comparative analysis of the translations produced by bilingual, multilingual and zero-shot systems; (ii) investigates the translation quality of two of the currently dominant neural architectures in MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively explores how the closeness between languages influences the zero-shot translation. Our analysis leverages multiple professional post-edits of automatic translations by several different systems and focuses both on automatic standard metrics (BLEU and TER) and on widely used error categories, which are lexical, morphology, and word order errors.},
	Author = {Surafel M. Lakew and Mauro Cettolo and Marcello Federico},
	Date-Added = {2020-07-17 00:05:54 +0200},
	Date-Modified = {2020-07-17 00:05:54 +0200},
	Eprint = {1806.06957},
	Month = {06},
	Title = {A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1806.06957.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODA2LjA2OTU3LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODA2LjA2OTU3LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgwNi4wNjk1Ny5wZGYADgAeAA4AMQA4ADAANgAuADAANgA5ADUANwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MDYuMDY5NTcucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1806.06957}}

@article{Ha:2016aa,
	Abstract = {In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.},
	Author = {Thanh-Le Ha and Jan Niehues and Alexander Waibel},
	Date-Added = {2020-07-16 00:09:28 +0200},
	Date-Modified = {2020-07-16 00:09:28 +0200},
	Eprint = {1611.04798},
	Month = {11},
	Title = {Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder},
	Url = {https://arxiv.org/pdf/1611.04798.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjExLjA0Nzk4LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjExLjA0Nzk4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYxMS4wNDc5OC5wZGYADgAeAA4AMQA2ADEAMQAuADAANAA3ADkAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MTEuMDQ3OTgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1611.04798}}

@article{Lakew:2019aa,
	Abstract = {Multilingual Neural Machine Translation (MNMT) for low-resource languages (LRL) can be enhanced by the presence of related high-resource languages (HRL), but the relatedness of HRL usually relies on predefined linguistic assumptions about language similarity. Recently, adapting MNMT to a LRL has shown to greatly improve performance. In this work, we explore the problem of adapting an MNMT model to an unseen LRL using data selection and model adaptation. In order to improve NMT for LRL, we employ perplexity to select HRL data that are most similar to the LRL on the basis of language distance. We extensively explore data selection in popular multilingual NMT settings, namely in (zero-shot) translation, and in adaptation from a multilingual pre-trained model, for both directions (LRL-en). We further show that dynamic adaptation of the model's vocabulary results in a more favourable segmentation for the LRL in comparison with direct adaptation. Experiments show reductions in training time and significant performance gains over LRL baselines, even with zero LRL data (+13.0 BLEU), up to +17.0 BLEU for pre-trained multilingual model dynamic adaptation with related data selection. Our method outperforms current approaches, such as massively multilingual models and data augmentation, on four LRL.},
	Author = {Surafel M. Lakew and Alina Karakanta and Marcello Federico and Matteo Negri and Marco Turchi},
	Date-Added = {2020-07-11 23:34:12 +0200},
	Date-Modified = {2020-07-11 23:34:12 +0200},
	Eprint = {1910.13998},
	Month = {10},
	Title = {Adapting Multilingual Neural Machine Translation to Unseen Languages},
	Url = {https://arxiv.org/pdf/1910.13998.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTEwLjEzOTk4LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTEwLjEzOTk4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkxMC4xMzk5OC5wZGYADgAeAA4AMQA5ADEAMAAuADEAMwA5ADkAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MTAuMTM5OTgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1910.13998}}

@article{Tan:2019aa,
	Abstract = {Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods},
	Author = {Xu Tan and Jiale Chen and Di He and Yingce Xia and Tao Qin and Tie-Yan Liu},
	Date-Added = {2020-07-11 23:30:15 +0200},
	Date-Modified = {2020-07-11 23:30:15 +0200},
	Eprint = {1908.09324},
	Month = {08},
	Title = {Multilingual Neural Machine Translation with Language Clustering},
	Url = {https://arxiv.org/pdf/1908.09324.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTA4LjA5MzI0LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTA4LjA5MzI0LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkwOC4wOTMyNC5wZGYADgAeAA4AMQA5ADAAOAAuADAAOQAzADIANAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MDguMDkzMjQucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1908.09324}}

@article{Bjerva_2019,
	Author = {Bjerva, Johannes and {\"O}stling, Robert and Veiga, Maria Han and Tiedemann, J{\"o}rg and Augenstein, Isabelle},
	Date-Added = {2020-07-11 23:29:37 +0200},
	Date-Modified = {2020-07-11 23:29:37 +0200},
	Doi = {10.1162/coli_a_00351},
	Issn = {1530-9312},
	Journal = {Computational Linguistics},
	Month = {Jun},
	Number = {2},
	Pages = {381--389},
	Publisher = {MIT Press - Journals},
	Title = {What Do Language Representations Really Represent?},
	Url = {http://dx.doi.org/10.1162/coli_a_00351},
	Volume = {45},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/coli_a_00351}}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@mastersthesis{Moss1437826,
	Abstract = {In this work, we test two novel methods of using word embeddings to detect lexical semantic change, attempting to overcome limitations associated with conventional approaches to this problem. Using a diachronic corpus spanning over a hundred years, we generate word embeddings for each decade with the intention of evaluating how meaning changes are represented in embeddings for the same word across time. Our approach differs from previous works in this field in that we encode words as probabilistic Gaussian distributions and bimodal probabilistic Gaussian mixtures, rather than conventional word vectors. We provide a discussion and analysis of our results, comparing the approaches we implemented with those used in previous works. We also conducted further analysis on whether additional information regarding the nature of semantic change could be discerned from particular qualities of the embeddings we generated for our experiments. In our results, we find that encoding words as probabilistic Gaussian embeddings can provide an enhanced degree of reliability with regard to detecting lexical semantic change. Furthermore, we are able to represent additional information regarding the nature of such changes through the variance of these embeddings. Encoding words as bimodal Gaussian mixtures however is generally unsuccessful for this task, proving to be not reliable enough at distinguishing between discrete senses to effectively detect and measure such changes. We provide potential explanations for the results we observe, and propose improvements that can be made to our approach to potentially improve performance. },
	Author = {Moss, Adam},
	Date-Added = {2020-07-11 23:05:56 +0200},
	Date-Modified = {2020-07-11 23:05:56 +0200},
	Institution = {Uppsala University, Department of Linguistics and Philology},
	Keywords = {historical linguistics, historical semantics, lexical semantic change, diachronic semantic change, word embeddings, probabilistic word embeddings, gaussian word embeddings},
	Pages = {45},
	School = {Uppsala University, Department of Linguistics and Philology},
	Title = {Detecting Lexical Semantic Change Using Probabilistic Gaussian Word Embeddings},
	Year = {2020},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxApLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9mdWxsdGV4dDAxMS5wZGZPEQFYAAAAAAFYAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8PZnVsbHRleHQwMTEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAC8vOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmZ1bGx0ZXh0MDExLnBkZgAADgAgAA8AZgB1AGwAbAB0AGUAeAB0ADAAMQAxAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgAtVXNlcnMvc2hpZmVpL0RvY3VtZW50cy9QYXBlcnMvZnVsbHRleHQwMTEucGRmAAATAAEvAAAVAAIADf//AAAACAANABoAJABQAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAaw=}}

@article{Lakew:2018aa,
	Abstract = {Recent work on multilingual neural machine translation reported competitive performance with respect to bilingual models and surprisingly good performance even on (zeroshot) translation directions not observed at training time. We investigate here a zero-shot translation in a particularly lowresource multilingual setting. We propose a simple iterative training procedure that leverages a duality of translations directly generated by the system for the zero-shot directions. The translations produced by the system (sub-optimal since they contain mixed language from the shared vocabulary), are then used together with the original parallel data to feed and iteratively re-train the multilingual network. Over time, this allows the system to learn from its own generated and increasingly better output. Our approach shows to be effective in improving the two zero-shot directions of our multilingual model. In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models. Further analysis shows that there is also a slight improvement in the non-zero-shot language directions.},
	Author = {Surafel M. Lakew and Quintino F. Lotito and Matteo Negri and Marco Turchi and Marcello Federico},
	Date-Added = {2020-07-11 22:58:06 +0200},
	Date-Modified = {2020-07-11 22:58:06 +0200},
	Eprint = {1811.01389},
	Month = {11},
	Title = {Improving Zero-Shot Translation of Low-Resource Languages},
	Url = {https://arxiv.org/pdf/1811.01389.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODExLjAxMzg5LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODExLjAxMzg5LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgxMS4wMTM4OS5wZGYADgAeAA4AMQA4ADEAMQAuADAAMQAzADgAOQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MTEuMDEzODkucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1811.01389}}

@article{Ha:2017aa,
	Abstract = {In this paper, we proposed two strategies which can be applied to a multilingual neural machine translation system in order to better tackle zero-shot scenarios despite not having any parallel corpus. The experiments show that they are effective in terms of both performance and computing resources, especially in multilingual translation of unbalanced data in real zero-resourced condition when they alleviate the language bias problem.},
	Author = {Thanh-Le Ha and Jan Niehues and Alexander Waibel},
	Date-Added = {2020-07-11 22:57:52 +0200},
	Date-Modified = {2020-07-11 22:57:52 +0200},
	Eprint = {1711.07893},
	Month = {11},
	Title = {Effective Strategies in Zero-Shot Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1711.07893.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzExLjA3ODkzLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzExLjA3ODkzLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcxMS4wNzg5My5wZGYADgAeAA4AMQA3ADEAMQAuADAANwA4ADkAMwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MTEuMDc4OTMucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1711.07893}}

@article{Arivazhagan:2019aa,
	Abstract = {Multilingual Neural Machine Translation (NMT) models are capable of translating between multiple source and target languages. Despite various approaches to train such models, they have difficulty with zero-shot translation: translating between language pairs that were not together seen during training. In this paper we first diagnose why state-of-the-art multilingual NMT models that rely purely on parameter sharing, fail to generalize to unseen language pairs. We then propose auxiliary losses on the NMT encoder that impose representational invariance across languages. Our simple approach vastly improves zero-shot translation quality without regressing on supervised directions. For the first time, on WMT14 English-FrenchGerman, we achieve zero-shot performance that is on par with pivoting. We also demonstrate the easy scalability of our approach to multiple languages on the IWSLT 2017 shared task.},
	Author = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Roee Aharoni and Melvin Johnson and Wolfgang Macherey},
	Date-Added = {2020-07-11 22:57:41 +0200},
	Date-Modified = {2020-07-11 22:57:41 +0200},
	Eprint = {1903.07091},
	Month = {03},
	Title = {The Missing Ingredient in Zero-Shot Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1903.07091.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTAzLjA3MDkxLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTAzLjA3MDkxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkwMy4wNzA5MS5wZGYADgAeAA4AMQA5ADAAMwAuADAANwAwADkAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MDMuMDcwOTEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1903.07091}}

@article{Tsvetkov:2016aa,
	Abstract = {We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually.},
	Author = {Yulia Tsvetkov and Sunayana Sitaram and Manaal Faruqui and Guillaume Lample and Patrick Littell and David Mortensen and Alan W Black and Lori Levin and Chris Dyer},
	Date-Added = {2020-07-06 23:26:36 +0200},
	Date-Modified = {2020-07-06 23:29:52 +0200},
	Eprint = {1605.03832},
	Month = {05},
	Title = {Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning},
	Url = {https://arxiv.org/pdf/1605.03832.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA1LjAzODMyLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA1LjAzODMyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNS4wMzgzMi5wZGYADgAeAA4AMQA2ADAANQAuADAAMwA4ADMAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDUuMDM4MzIucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1605.03832}}

@inproceedings{malaviya-etal-2017-learning,
	Abstract = {One central mystery of neural NLP is what neural models {``}know{''} about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages? Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one NMT system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages geographic and phylogenetic neighbors.},
	Address = {Copenhagen, Denmark},
	Author = {Malaviya, Chaitanya and Neubig, Graham and Littell, Patrick},
	Booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2020-06-23 00:07:04 +0200},
	Date-Modified = {2020-06-23 00:07:04 +0200},
	Doi = {10.18653/v1/D17-1268},
	Month = sep,
	Pages = {2529--2535},
	Publisher = {Association for Computational Linguistics},
	Title = {Learning Language Representations for Typology Prediction},
	Url = {https://www.aclweb.org/anthology/D17-1268.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9kMTctMTI2OC5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MZDE3LTEyNjgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmQxNy0xMjY4LnBkZgAOABoADABkADEANwAtADEAMgA2ADgALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9kMTctMTI2OC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D17-1268}}

@article{Smith:2017aa,
	Abstract = {Usually bilingual word vectors are trained "online". Mikolov et al. showed they can also be found "offline", whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel "inverted softmax" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34\% to 43\%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a "pseudo-dictionary" from the identical character strings which appear in both languages, achieving 40\% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68\%.},
	Author = {Samuel L. Smith and David H. P. Turban and Steven Hamblin and Nils Y. Hammerla},
	Date-Added = {2020-06-18 17:03:21 +0200},
	Date-Modified = {2020-06-18 17:10:12 +0200},
	Eprint = {1702.03859},
	Month = {02},
	Title = {Offline bilingual word vectors, orthogonal transformations and the inverted softmax},
	Url = {https://arxiv.org/pdf/1702.03859.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzAyLjAzODU5LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzAyLjAzODU5LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcwMi4wMzg1OS5wZGYADgAeAA4AMQA3ADAAMgAuADAAMwA4ADUAOQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MDIuMDM4NTkucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1702.03859}}

@article{Johnson:2016aa,
    title = "{G}oogle{'}s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
    author = "Johnson, Melvin  and
      Schuster, Mike  and
      Le, Quoc V.  and
      Krikun, Maxim  and
      Wu, Yonghui  and
      Chen, Zhifeng  and
      Thorat, Nikhil  and
      Vi{\'e}gas, Fernanda  and
      Wattenberg, Martin  and
      Corrado, Greg  and
      Hughes, Macduff  and
      Dean, Jeffrey",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1024",
    doi = "10.1162/tacl_a_00065",
    pages = "339--351",
    abstract = "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT{'}14 benchmarks, a single multilingual model achieves comparable performance for English鈫扚rench and surpasses state-of-theart results for English鈫扜erman. Similarly, a single multilingual model surpasses state-of-the-art results for French鈫扙nglish and German鈫扙nglish on WMT{'}14 and WMT{'}15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",
}

@inproceedings{vulic-moens-2013-study,
	Address = {Seattle, Washington, USA},
	Author = {Vuli{\'c}, Ivan and Moens, Marie-Francine},
	Booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2020-06-13 00:59:24 +0200},
	Date-Modified = {2020-06-13 00:59:24 +0200},
	Month = oct,
	Pages = {1613--1624},
	Publisher = {Association for Computational Linguistics},
	Title = {A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)},
	Url = {https://www.aclweb.org/anthology/D13-1168.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9kMTMtMTE2OC5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MZDEzLTExNjgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmQxMy0xMTY4LnBkZgAOABoADABkADEAMwAtADEAMQA2ADgALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9kMTMtMTE2OC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D13-1168}}

@article{Hermann:2013aa,
	Abstract = {Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.},
	Author = {Karl Moritz Hermann and Phil Blunsom},
	Date-Added = {2020-06-13 00:57:25 +0200},
	Date-Modified = {2020-06-13 00:57:25 +0200},
	Eprint = {1312.6173},
	Month = {12},
	Title = {Multilingual Distributed Representations without Word Alignment},
	Url = {https://arxiv.org/pdf/1312.6173.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzEyLjYxNzMucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMTIuNjE3My5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzEyLjYxNzMucGRmAAAOABwADQAxADMAMQAyAC4ANgAxADcAMwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMTIuNjE3My5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1312.6173}}

@inproceedings{Kim:2019aa,
    title = "Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies",
    author = "Kim, Yunsu  and
      Gao, Yingbo  and
      Ney, Hermann",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1120",
    doi = "10.18653/v1/P19-1120",
    pages = "1246--1257",
    abstract = "Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1{\%} BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer.",
}

@article{Baziotis:2020aa,
	Abstract = {The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM "disagrees" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.},
	Author = {Christos Baziotis and Barry Haddow and Alexandra Birch},
	Date-Added = {2020-06-03 18:26:35 +0200},
	Date-Modified = {2020-06-03 18:26:35 +0200},
	Eprint = {2004.14928},
	Month = {04},
	Title = {Language Model Prior for Low-Resource Neural Machine Translation},
	Url = {https://arxiv.org/pdf/2004.14928.pdf},
	Year = {2020},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8yMDA0LjE0OTI4LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4yMDA0LjE0OTI4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MjAwNC4xNDkyOC5wZGYADgAeAA4AMgAwADAANAAuADEANAA5ADIAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzIwMDQuMTQ5MjgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/2004.14928}}

@article{Mikolov:2013ac,
	Abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
	Author = {Tomas Mikolov and Quoc V. Le and Ilya Sutskever},
	Date-Added = {2020-06-01 16:44:59 +0200},
	Date-Modified = {2020-06-13 00:18:49 +0200},
	Eprint = {1309.4168},
	Month = {09},
	Title = {Exploiting Similarities among Languages for Machine Translation},
	Url = {https://arxiv.org/pdf/1309.4168.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzA5LjQxNjgucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMDkuNDE2OC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzA5LjQxNjgucGRmAAAOABwADQAxADMAMAA5AC4ANAAxADYAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMDkuNDE2OC5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1309.4168}}

@article{levy-etal-2015-improving,
	Abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
	Author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
	Date-Added = {2020-05-29 16:13:23 +0200},
	Date-Modified = {2020-05-29 16:13:23 +0200},
	Doi = {10.1162/tacl_a_00134},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {211--225},
	Title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
	Url = {https://www.aclweb.org/anthology/Q15-1016.pdf},
	Volume = {3},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9xMTUtMTAxNi5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8McTE1LTEwMTYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOnExNS0xMDE2LnBkZgAOABoADABxADEANQAtADEAMAAxADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9xMTUtMTAxNi5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/Q15-1016}}

@article{Ruder:2019aa,
	Abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
	Author = {Sebastian Ruder and Ivan Vuli{\'c} and Anders S{\o}gaard},
	Date-Added = {2020-05-26 15:52:14 +0200},
	Date-Modified = {2020-05-26 15:52:14 +0200},
	Doi = {10.1613/jair.1.11640},
	Eprint = {1706.04902},
	Journal = {JAIR},
	Pages = {569-631},
	Title = {A Survey Of Cross-lingual Word Embedding Models},
	Url = {https://arxiv.org/pdf/1706.04902.pdf},
	Volume = {65},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzA2LjA0OTAyLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzA2LjA0OTAyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcwNi4wNDkwMi5wZGYADgAeAA4AMQA3ADAANgAuADAANAA5ADAAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MDYuMDQ5MDIucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1706.04902},
	Bdsk-Url-2 = {https://doi.org/10.1613/jair.1.11640}}

@inproceedings{Qi:2018aa,
    title = "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
    author = "Qi, Ye  and
      Sachan, Devendra  and
      Felix, Matthieu  and
      Padmanabhan, Sarguna  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-2084",
    doi = "10.18653/v1/N18-2084",
    pages = "529--535",
    abstract = "The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases {--} providing gains of up to 20 BLEU points in the most favorable setting.",
}

@inproceedings{artetxe-etal-2017-learning,
	Abstract = {Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.},
	Address = {Vancouver, Canada},
	Author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
	Booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Date-Added = {2020-05-23 23:04:49 +0200},
	Date-Modified = {2020-05-23 23:04:49 +0200},
	Doi = {10.18653/v1/P17-1042},
	Month = jul,
	Pages = {451--462},
	Publisher = {Association for Computational Linguistics},
	Title = {Learning bilingual word embeddings with (almost) no bilingual data},
	Url = {https://www.aclweb.org/anthology/P17-1042.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9wMTctMTA0Mi5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8McDE3LTEwNDIucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOnAxNy0xMDQyLnBkZgAOABoADABwADEANwAtADEAMAA0ADIALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9wMTctMTA0Mi5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P17-1042}}

@inproceedings{zou-etal-2013-bilingual,
	Abstract = {We introduce bilingual word embeddings: se- mantic embeddings associated across two lan- guages in the context of neural language mod- els. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to con- strain translational equivalence. The new em- beddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual em- beddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.},
	Address = {Seattle, Washington, USA},
	Author = {Zou, Will Y. and Socher, Richard and Cer, Daniel and Manning, Christopher D.},
	Booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2020-05-23 22:40:38 +0200},
	Date-Modified = {2020-06-13 00:04:12 +0200},
	Month = oct,
	Pages = {1393--1398},
	Publisher = {Association for Computational Linguistics},
	Title = {Bilingual Word Embeddings for Phrase-Based Machine Translation},
	Url = {https://www.aclweb.org/anthology/D13-1141.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9kMTMtMTE0MS5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MZDEzLTExNDEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmQxMy0xMTQxLnBkZgAOABoADABkADEAMwAtADEAMQA0ADEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9kMTMtMTE0MS5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D13-1141}}

@inproceedings{Conneau:2017aa,
  author    = {Guillaume Lample and
               Alexis Conneau and
               Marc'Aurelio Ranzato and
               Ludovic Denoyer and
               Herv{\'{e}} J{\'{e}}gou},
  title     = {Word translation without parallel data},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=H196sainb},
  timestamp = {Thu, 25 Jul 2019 14:25:59 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LampleCRDJ18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Upadhyay:2016aa,
	Abstract = {Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.},
	Author = {Shyam Upadhyay and Manaal Faruqui and Chris Dyer and Dan Roth},
	Date-Added = {2020-05-18 11:57:35 +0200},
	Date-Modified = {2020-05-18 11:57:35 +0200},
	Eprint = {1604.00425},
	Month = {04},
	Title = {Cross-lingual Models of Word Embeddings: An Empirical Comparison},
	Url = {https://arxiv.org/pdf/1604.00425.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA0LjAwNDI1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA0LjAwNDI1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNC4wMDQyNS5wZGYADgAeAA4AMQA2ADAANAAuADAAMAA0ADIANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDQuMDA0MjUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1604.00425}}

@article{bengio2003neural,
	Author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
	Date-Added = {2020-05-18 11:43:11 +0200},
	Date-Modified = {2020-07-09 23:57:27 +0200},
	Issn = {1532-4435},
	Issue_Date = {3/1/2003},
	Journal = {J. Mach. Learn. Res.},
	Month = mar,
	Number = {null},
	Numpages = {19},
	Pages = {1137--1155},
	Publisher = {JMLR.org},
	Title = {A Neural Probabilistic Language Model},
	Volume = {3},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxArLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy85NDQ5MTkuOTQ0OTY2LnBkZk8RAWAAAAAAAWAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xE5NDQ5MTkuOTQ0OTY2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAMS86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6OTQ0OTE5Ljk0NDk2Ni5wZGYAAA4AJAARADkANAA0ADkAMQA5AC4AOQA0ADQAOQA2ADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAC9Vc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy85NDQ5MTkuOTQ0OTY2LnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQAUgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAG2},
	Bdsk-Url-1 = {https://dl.acm.org/doi/10.5555/944919.944966}}

@inproceedings{klementiev-etal-2012-inducing,
	Abstract = {Distributed representations of words have proven extremely useful in numerous natural lan- guage processing tasks. Their appeal is that they can help alleviate data sparsity problems common to supervised learning. Methods for inducing these representations require only unlabeled language data, which are plentiful for many natural languages. In this work, we induce distributed representations for a pair of languages jointly. We treat it as a multitask learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. These representations can be used for a number of crosslingual learning tasks, where a learner can be trained on annotations present in one language and applied to test data in another. We show that our representations are informative by using them for crosslingual document classification, where classifiers trained on these representations substantially outperform strong baselines (e.g. machine translation) when applied to a new language.},
	Address = {Mumbai, India},
	Author = {Klementiev, Alexandre and Titov, Ivan and Bhattarai, Binod},
	Booktitle = {Proceedings of {COLING} 2012},
	Date-Added = {2020-05-18 11:24:23 +0200},
	Date-Modified = {2020-05-18 11:31:20 +0200},
	Month = dec,
	Pages = {1459--1474},
	Publisher = {The COLING 2012 Organizing Committee},
	Title = {Inducing Crosslingual Distributed Representations of Words},
	Url = {https://www.aclweb.org/anthology/C12-1089.pdf},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9DMTItMTA4OS5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MQzEyLTEwODkucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOkMxMi0xMDg5LnBkZgAOABoADABDADEAMgAtADEAMAA4ADkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9DMTItMTA4OS5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/C12-1089}}

@article{Ammar:2016aa,
	Abstract = {We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC-CCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.},
	Author = {Waleed Ammar and George Mulcaire and Yulia Tsvetkov and Guillaume Lample and Chris Dyer and Noah A. Smith},
	Date-Added = {2020-05-18 11:14:22 +0200},
	Date-Modified = {2020-05-18 11:14:22 +0200},
	Eprint = {1602.01925},
	Month = {02},
	Title = {Massively Multilingual Word Embeddings},
	Url = {https://arxiv.org/pdf/1602.01925.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjAyLjAxOTI1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjAyLjAxOTI1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwMi4wMTkyNS5wZGYADgAeAA4AMQA2ADAAMgAuADAAMQA5ADIANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDIuMDE5MjUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1602.01925}}

@article{Bojanowski:2016aa,
    title = "Enriching Word Vectors with Subword Information",
    author = "Bojanowski, Piotr  and
      Grave, Edouard  and
      Joulin, Armand  and
      Mikolov, Tomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1010",
    doi = "10.1162/tacl_a_00051",
    pages = "135--146",
    abstract = "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
}

@inproceedings{Joulin:2018aa,
    title = "Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion",
    author = "Joulin, Armand  and
      Bojanowski, Piotr  and
      Mikolov, Tomas  and
      J{\'e}gou, Herv{\'e}  and
      Grave, Edouard",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1330",
    doi = "10.18653/v1/D18-1330",
    pages = "2979--2984",
    abstract = "Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a quadratic problem to learn a orthogonal matrix aligning a bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.",
}

@inproceedings{ri2019multilingual,
	Author = {Ryokan Ri and Yoshimasa Tsuruoka},
	Booktitle = {:},
	Date-Added = {2020-05-16 11:38:32 +0200},
	Date-Modified = {2020-05-18 11:47:50 +0200},
	Title = {What do Multilingual Neural Machine Translation Models learn about Typology?},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAfLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy82LnBkZk8RATAAAAAAATAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////wU2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAJS86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6Ni5wZGYAAA4ADAAFADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACNVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy82LnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQARgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAF6}}

@inproceedings{Basirat1347044,
	Author = {Basirat, Ali and de Lhoneux, Miryam and Kulmizev, Artur and Kurfal, Murathan and Nivre, Joakim and {\"O}stling, Robert},
	Booktitle = {:},
	Date-Added = {2020-05-16 10:41:34 +0200},
	Date-Modified = {2020-05-16 10:41:34 +0200},
	Institution = {Department of Linguistics, Stockholm University},
	Title = {Polyglot Parsing for One Thousand and One Languages (And Then Some)},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxApLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9GVUxMVEVYVDAxMC5wZGZPEQFYAAAAAAFYAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8PRlVMTFRFWFQwMTAucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAC8vOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOkZVTExURVhUMDEwLnBkZgAADgAgAA8ARgBVAEwATABUAEUAWABUADAAMQAwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgAtVXNlcnMvc2hpZmVpL0RvY3VtZW50cy9QYXBlcnMvRlVMTFRFWFQwMTAucGRmAAATAAEvAAAVAAIADf//AAAACAANABoAJABQAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAaw=},
	Bdsk-Url-1 = {http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-392156}}

@article{Al-Rfou:2013aa,
	Abstract = {Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.},
	Author = {Rami Al-Rfou and Bryan Perozzi and Steven Skiena},
	Date-Added = {2020-05-16 10:41:08 +0200},
	Date-Modified = {2020-05-16 10:41:08 +0200},
	Eprint = {1307.1662},
	Month = {07},
	Title = {Polyglot: Distributed Word Representations for Multilingual NLP},
	Url = {https://arxiv.org/pdf/1307.1662.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzA3LjE2NjIucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMDcuMTY2Mi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzA3LjE2NjIucGRmAAAOABwADQAxADMAMAA3AC4AMQA2ADYAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMDcuMTY2Mi5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1307.1662}}

@mastersthesis{Dyer1365879,
	Abstract = {Cross-lingual word embeddings are an increasingly important reseource in cross-lingual methods for NLP, particularly for their role in transfer learning and unsupervised machine translation, purportedly opening up the opportunity for NLP applications for low-resource languages.  However, most research in this area implicitly expects the availablility of vast monolingual corpora for training embeddings, a scenario which is not realistic for many of the world's languages.  Moreover, much of the reporting of the performance of cross-lingual word embeddings is based on a fairly narrow set of mostly European language pairs.  Our study examines the performance of cross-lingual alignment across a more diverse set of language pairs; controls for the effect of the corpus size on which the monolingual embedding spaces are trained; and studies the impact of spectral graph properties of the embedding spsace on alignment.  Through our experiments on a more diverse set of language pairs, we find that performance in bilingual lexicon induction is generally poor in heterogeneous pairs, and that even using a gold or heuristically derived dictionary has little impact on the performance on these pairs of languages.  We also find that the performance for these languages only increases slowly with corpus size.  Finally, we find a moderate correlation between the isospectral difference of the source and target embeddings and the performance of bilingual lexicon induction.  We infer that methods other than cross-lingual alignment may be more appropriate in the case of both low resource languages and heterogeneous language pairs. },
	Author = {Dyer, Andrew},
	Date-Added = {2020-05-16 10:25:31 +0200},
	Date-Modified = {2020-05-16 10:25:31 +0200},
	Institution = {Uppsala University, Department of Linguistics and Philology},
	Keywords = {word embeddings, cross-lingual, multilingual, low-resource, corpus size, Vecmap, FastText, alignment, orthogonal, eigenvalues, Laplacian, isospectral, isomorphic, bilingual lexicon induction},
	Pages = {53},
	School = {Uppsala University, Department of Linguistics and Philology},
	Title = {Low Supervision, Low Corpus size, Low Similarity! Challenges in cross-lingual alignment of word embeddings : An exploration of the limitations of cross-lingual word embedding alignment in truly low resource scenarios},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9GVUxMVEVYVDAxLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w5GVUxMVEVYVDAxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6RlVMTFRFWFQwMS5wZGYADgAeAA4ARgBVAEwATABUAEUAWABUADAAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzL0ZVTExURVhUMDEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-395946}}

@article{Mikolov:2013ab,
	Abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	Author = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
	Date-Added = {2020-05-16 10:21:50 +0200},
	Date-Modified = {2020-06-13 00:04:14 +0200},
	Eprint = {1310.4546},
	Month = {10},
	Title = {Distributed Representations of Words and Phrases and their Compositionality},
	Url = {https://arxiv.org/pdf/1310.4546.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzEwLjQ1NDYucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMTAuNDU0Ni5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzEwLjQ1NDYucGRmAAAOABwADQAxADMAMQAwAC4ANAA1ADQANgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMTAuNDU0Ni5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1310.4546}}

@article{Mikolov:2013aa,
	Abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	Author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	Date-Added = {2020-05-16 10:21:12 +0200},
	Date-Modified = {2020-06-13 00:04:08 +0200},
	Eprint = {1301.3781},
	Month = {01},
	Title = {Efficient Estimation of Word Representations in Vector Space},
	Url = {https://arxiv.org/pdf/1301.3781.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzAxLjM3ODEucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMDEuMzc4MS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzAxLjM3ODEucGRmAAAOABwADQAxADMAMAAxAC4AMwA3ADgAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMDEuMzc4MS5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1301.3781}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>Language Embeddings</string>
		<key>keys</key>
		<string>littell-etal-2017-uriel,malaviya-etal-2017-learning,Bjerva_2019</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Machine Translation</string>
		<key>keys</key>
		<string>Johnson:2016aa,Conneau:2017aa,Kim:2019aa,Baziotis:2020aa,Ha:2017aa,Lakew:2018aa,Arivazhagan:2019aa,Lakew:2019aa,Ha:2016aa,Lakew:2018ab,Firat:2016aa,Blackwood:2018aa,Vaswani:2017aa,Koehn:2017aa,Nguyen:2017aa,Zoph:2016aa,Arthur:2016aa</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Word Embeddings</string>
		<key>keys</key>
		<string>bengio2003neural,klementiev-etal-2012-inducing,Mikolov:2013aa,Al-Rfou:2013aa,vulic-moens-2013-study,zou-etal-2013-bilingual,Mikolov:2013ab,Hermann:2013aa,levy-etal-2015-improving,Ammar:2016aa,Upadhyay:2016aa,Bojanowski:2016aa,Smith:2017aa,artetxe-etal-2017-learning,Joulin:2018aa,Qi:2018aa,Ruder:2019aa,Dyer1365879,Artetxe:2017aa,neishi-etal-2017-bag,inproceedings,Heinzerling:2019aa,glorot2010understanding,Mikolov:2013ac</string>
	</dict>
</array>
</plist>
}}
