\documentclass[thesis,fonts=libertine]{cluu}

\usepackage[style=cluu]{biblatex}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{siunitx}
\usepackage[boxed, noline]{algorithm2e}
\usepackage{pgfplots}

\graphicspath{ {figures/} }
\addbibresource{thesis.bib}

\usepackage{pythonhighlight}    % for \inputpython at the end

\begin{document}
\author{Shifei Chen}
\supervisors{Ali Basirat, Uppsala University}
\title{Cross-lingual Word Embeddings Beyond Zero-shot Machine Translation}

\maketitle

\begin{abstract}
  The concept of \emph{palindromes} is introduced, and some method for
  finding palindromes is developed.
\end{abstract}

\tableofcontents

\addchap{Preface}

This thesis was finished under the supervision of Ali Basirat. I would like 
to thank him first for his guidance, inspiration and passion.

The Saga supercomputer \footnote{\url{https://www.sigma2.no/systems\#saga}} owned by UNINETT Sigma2 hosted all of the experiment in this thesis. Without it this thesis would not be possible.

Thank you Mr. Anders Wall and everyone in the Anders Wall Scholarship Foundation for sponsoring my Master's study. This opportunity led me to meet everyone in the Master Programme in Language Technology, from whom I have learned a lot during the 2-years journey.

Last but not least, I would like to say a thank you to my parents for their unconditional love and support; to all of my friends for the unique memories we have created; and to my girlfriend, who has always been next to me when the virus made everything unusual.

% by using \addchap instead of \chapter this preface isn't numbered.

\chapter{Introduction}

Palindromes are fun. I've tried to find some.
In Chapter \ref{chap:prev} previous work is reviewed, and
Chapter \ref{chap:results} is about my results.

\chapter{Background}
\label{chap:background}

\section{Word Embeddings}
\subsection{Representing Words in Vectors}

In Natural Language Processing, people need to convert the natural representation of words into forms that are more efficient for computers to process. The idea started with statistical language modeling introduced by \cite{bengio2003neural}. In 2013, \cite{Mikolov:2013aa} introduced Word2Vec, which encapsulates words and their latent information into vectors. Besides the benefit that it simplifies representation and storage of words for computers, it also enables the possibilities to calculate words and their semantic meanings just as vectors.

Take an example of vocabulary $V=\{\text{king}, \text{queen}, \text{man}, \text{woman}\}$, if we convert these words into vectors such as 

\begin{align*}
  \vec{k} &= \text{vec}(\text{king})\\
  \vec{q} &= \text{vec}(\text{queen})\\
  \vec{m} &= \text{vec}(\text{man})\\
  \vec{w} &= \text{vec}(\text{woman})
\end{align*}

We could have an equation of 

\begin{equation}
  \label{equa:semantic_vectors}
  \vec{q}=\vec{k}-\vec{m}+\vec{w}
\end{equation}

It is meaningful from both the mathematical perspective and the linguistic perspective. Figure \ref{fig:semantic_vectors} illustrates both perspectives in a vector space that contains these four vectors. Geometrically, the angle between $\vec{k}$ and $\vec{q}$ is small, together with the angle between $\vec{m}$ and $\vec{w}$. From the cosine similarity definition below

\begin{equation*}
  sim(x, y) = \cos(\theta) = \frac{x \cdot y}{||x||||y||}
\end{equation*}

It indicates that the cosine similarities of every two of them are high. Besides, Equation \ref{equa:semantic_vectors} is correct in this vector space too. We could get such an equation from basic vector calculation definitions.

\begin{figure}
  \centering
  \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt
  \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
    %uncomment if require: \path (0,256); %set diagram left start at 0, and has height of 256

    %Shape: Axis 2D [id:dp40983147480798976]
    \draw  (49,221.35) -- (260.5,221.35)(70.15,31) -- (70.15,242.5) (253.5,216.35) -- (260.5,221.35) -- (253.5,226.35) (65.15,38) -- (70.15,31) -- (75.15,38)  ;
    %Straight Lines [id:da3468248594605029]
    \draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ]   (70.15,221.35) -- (91.04,133.95) ;
    \draw [shift={(91.5,132)}, rotate = 463.44] [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
    %Straight Lines [id:da060011766798584776]
    \draw [color={rgb, 255:red, 80; green, 227; blue, 194 }  ,draw opacity=1 ][fill={rgb, 255:red, 182; green, 34; blue, 34 }  ,fill opacity=1 ]   (70.15,221.35) -- (98.61,163.79) ;
    \draw [shift={(99.5,162)}, rotate = 476.31] [color={rgb, 255:red, 80; green, 227; blue, 194 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
    %Straight Lines [id:da268618270283273]
    \draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ]   (70.15,221.35) -- (127.85,182.12) ;
    \draw [shift={(129.5,181)}, rotate = 505.79] [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
    %Straight Lines [id:da46956564149919644]
    \draw [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ]   (70.15,221.35) -- (131.69,192.84) ;
    \draw [shift={(133.5,192)}, rotate = 515.14] [color={rgb, 255:red, 245; green, 166; blue, 35 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

    % Text Node
    \draw (83,108) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\vec{k}$};
    % Text Node
    \draw (101,142) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\vec{q}$};
    % Text Node
    \draw (132,166) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\vec{m}$};
    % Text Node
    \draw (138,183) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\vec{w}$};
  \end{tikzpicture}
  \caption{Illustraion of a vector space where Equation \ref{equa:semantic_vectors} exists.}
  \label{fig:semantic_vectors}
\end{figure}

Semantically, the word ``king'' and ``queue'' are closely related in the same category as well as the word ``man'' and ``woman''. It is meaningful to say ``queue'' is a ``king'' being replaced its ``man'' part by a female.

To turn words into vectors, one could use a simple one-hot encoding. Like in the example above we could make $\vec{k}=[1, 0, 0, 0]$. However, these one-hot vectors can merely capture any latent semantic information between different words. Recent vectorized word representations, or word embeddings, were learned through neural networks, such as Word2Vec, which learns word embeddings through a Skip-gram model or a Continuous Bag of Words model \parencite{Mikolov:2013ab}. Both models are shown in Figure \ref{fig:skip_gram_and_cbow}.

\begin{figure}
  \label{fig:skip_gram_and_cbow}
  \centering
  \includegraphics[width=0.8\textwidth]{skip_gram_and_cbow_models.png}
  \caption{The Skip-gram and the CBOW model. \parencite{Mikolov:2013ac}}
\end{figure}

\subsubsection{The Skip-gram model}

When given a target word $w$, the model can produce vector representations that are good at predicting the words surrounding $w$ within the context size of $C$. The probability of a context word $w_k$ given a target word $w$ is:

\begin{equation}
  P(w_k|w)=\frac{\exp({v_{w_k}^\prime}^\intercal v_w)}{\sum_{i=1}^{|V|}\exp({v^\prime_i}^\intercal v_w)}
\end{equation}

Here $|V|$ means the size of the whole vocabulary from the corpus. Both $v^\prime$ and $v$ stand for the input vector representation, and the output vector representation of a word \parencite{Mikolov:2013aa}. The aforementioned one-hot vectors are used in the Skip-gram model too. They can initialize the input representation $v^\prime$.

\subsubsection{The Continuous Bag of Words model (CBOW)}

The other model, CBOW, works just like the other side of the coin. It predicts the target word $w$ based on a bunch of context words $w_{-C}, w_{-C+1} ..., w_{C-1}, w_C$ within the window size $C$, as the formula below:

\begin{equation}
  P(w|w_{-C}, w_{-C+1} ..., w_{C-1}, w_C)=\frac{\exp({v^\prime_w}^\intercal \bar{v}_{w_k})}{\sum^{|V|}_{i=1}\exp({v^\prime_{w_i}}^\intercal \bar{v}_{w_k})}
\end{equation}

Here $\bar{v}_{w_k}$ means the sum of the context word $w_k$'s vectorized representation, while $v^\prime_w$ means the input vector representations of word $w$ as in the Skip-gram model.

The difference between these two models is that the CBOW model predicts the target word from multiple given context words, while the Skip-gram model predicts the context words from one given center word. Hence the Skip-gram model is better at predicting rare words because all of the words are treated equally in the \textit{word AND context} relationship. While in the CBOW model, frequent words have advantages over rare words as they will have higher probabilities in a given context. The Skip-gram model is arguably the most popular method to learn word embeddings as it is both fast and robust, especially with less frequent words in the corpus. \parencite{levy-etal-2015-improving}

\subsection{Multilingual Word Embeddings}
\label{sec:multilingual_word_embeddings}

Learned from approaches like the Skip-gram model or the CBOW model, vectorized word representations tend to cluster words with similar semantics \parencite{Mikolov:2013ac}. It then becomes attractive to see whether we could fit two or more languages into the same vector space. Word embeddings that consist of more than one language are called multilingual word embeddings.

\begin{figure}
  \label{fig:vec_space_align}
  \centering
  \includegraphics[width=0.8\textwidth]{vector_spaces_alignment.png}
  \caption{Aligning bilingual vector spaces. \parencite{Conneau:2017aa}}
\end{figure}

In the multilingual scenario, it is vital to align words in two different vector spaces to make word embeddings from different languages comparable. Figure \ref{fig:vec_space_align} illustrated the alignment method from \cite{Conneau:2017aa}. Suppose there is a set of word pairs in their associated vectorized representation $\{x_i, y_i\}_{i\in \{1, ..., n\}}$, the two vector spaces were aligned by a rotation matrix $W \in \mathbb{R}^{d \times d}$ as shown in process \textbf{(B)}, where we try to optimize the formula 

\begin{equation*}
  \min_{W \in \mathbb{R}^{d \times d}} \frac{1}{n}\sum_{i=1}^n \ell(Wx_i, y_i)
\end{equation*}

Here $\ell$ is the loss function and it is usually the square loss function $\ell_2(x, y)=||x-y||^2$. Then $W$ is further refined in process \textbf{(C)}, where we choose frequent words as anchor points and minimize the distance between each correspondent anchor points by an energy function. After this, the refined $W$ is then used to map all words in the dictionary during the inference process. We obtain the translation $t(i)$ of a given source word $i$ in the formula

\begin{equation*}
  t(i) \in \argmin_{j\in \{1, ..., N\}} \ell(Wx_i, y_j)
\end{equation*}

Again, the loss function $\ell$ is typically the square loss function. However, using the square loss function could make the model suffer from the ``hubness problem''. \cite{Conneau:2017aa} counter reacted to the ``hubness'' problem by introducing the cross-domain similarity local scaling (CSLS), whose detail is beyond this thesis's scope.

Despite the theoretical feasibility proven by mathematic formulas, we need data to drive the alignment process. Take bilingual word embeddings as the first step, their initial alignment data for adversarial learning of the rotation matrix $W$ could come from a bilingual dictionary \parencite{Mikolov:2013ac}. Also, there are other kinds of alignment using aligned data from sentence level, or even document level, even though word-level information is most common. By using word-level information, we can start with a pivot language (usually English) and map each other monolingual word embeddings by looking up translation dictionaries. This mapping process could also start with vectors only, where we choose a bilingual word embedding that shares a language (also typically English) with other bilingual embeddings and choose the other bilingual word embeddings by aligning their shared language subspace. Sentence-level parallel data is similar to the corpus in Machine Translation (MT), which contains sentence-aligned texts \parencite{Hermann:2013aa}. Document-level information is more common in the form of topic-aligned or class-aligned, such as Wikipedia data \parencite{vulic-moens-2013-study}. The alignment process of multilingual word embeddings is roughly the same as bilingual word embeddings, using parallel data from either word-level, sentence-level, or document-level \parencite{Ruder:2019aa}.

\subsection{fastText}
\label{sec:fasttext}

In this work, the author have chosen fastText aligned word vectors\footnote{\url{https://fasttext.cc/docs/en/aligned-vectors.html}} \parencite{Joulin:2018aa} as my vectorized word representation. They are based on the pre-trained vectors computed from the Wikipedia corpus using fastText \parencite{Bojanowski:2016aa}. Not only because of the soon to be talked good performance, but fastText aligned word embeddings also has a large selection of languages available to use out of the box. It is ideal for implementing the cross-lingual experiments on aligned word embeddings.

fastText is an extension of the original Word2Vec methods, which uses sub-words to augment low-frequency and unseen words. Take word \texttt{low-key} as an example. As a whole word, its possibility in a given document would be much lower than its components, \texttt{low} and \texttt{key}. fastText learns its vectorized representation from a smaller n-gram sub-word level. It divides the whole word into sub-words units as below if we assume $n=3$

\begin{equation*}
  \mathtt{\text{<}lo, low, ow\text{-}, w\text{-}k, \text{-}ke, key, ey\text{>}}
\end{equation*}

Each sub-word has its own vectorized representation learned through a CBOW or Skip-gram model as in Word2Vec. The word vector for the whole word unit \texttt{<low-key>} is then the sum of all of its sub-word units' vectors. Hence its rareness would be compensated by two more frequent subwords \texttt{low} and \texttt{key}, even if it might not appear in the training document at all.

In terms of multilingual alignment, fastText improves the standard solution to the hubness problem by directly including the Relaxed CSLS (RCSLS) criterion into the model during both the learning and the inference phase. Before the work of \cite{Joulin:2018aa}, inverted softmax (ISF) \cite{Smith:2017aa} or CSLS \parencite{Conneau:2017aa} was only used in the inference time to address the hubness problem while square loss is still the loss function used in the training time. However, since both the ISF and the CSLS are not consistent with the square loss function in the training time, they will create a discrepancy between the learning of the translation model and the inference. According to the authors, fastText outperforms other alignment approaches by 3 to 4\% on average.

\section{Multilingual Neural Machine Translation (MNMT) Systems}

\subsection{Multilingual Neural Machine Translation}

Neural Machine Translation (NMT) uses neural networks to learn the translation relationship between a source and a target language. Its power has gone over the traditional Statistical Machine Translation (SMT) and enabled things that were impossible in the past. One of that is Multilingual Neural Machine Translation, as shown in the Figure \ref{fig:google_mnmt}, it uses the same attentional encoder-decoder model as bilingual NMT but trains it on a multilingual corpus \parencite{Johnson:2016aa}. The benefit of such a multilingual system does not necessarily stop at higher translation performance between common languages like English, French, or Spanish; it also leverages additional information from high resource languages to low resource languages in the same semantic space \parencite{Ha:2016aa}. People identify this information leverage as a special form of transfer learning \parencite{Zoph:2016aa}, which could happen both in the horizontal or vertical direction \parencite{Lakew:2019aa}: In the horizontal direction, knowledge transfers from pre-trained data (such as word embeddings or language models) to the raw test data; in the vertical direction, knowledge transfers from languages to languages, which could either transfer from high resource to low resource languages or from seen languages to unseen languages. The latter is called zero-shot translation.

\begin{figure}
  \label{fig:google_mnmt}
  \centering
  \includegraphics[width=0.8\textwidth]{google_mnmt_architecture.png}
  \caption{Google's MNMT Architecturef from \parencite{Johnson:2016aa}}. 
\end{figure}

\subsection{Zero-shot Machine Translation Systems}
\label{sec:zero_shot_mt}

Zero-shot translation stands for translation between language pairs invisible to the MNMT system during the training time. E.g., we build an MNMT system with training language pairs of German-English and French-English while test its performance on a German-French scenario. In 2016, \cite{Johnson:2016aa} first published their result on a zero-shot MT system. Their multilingual MT system, including an encoder, decoder, and attention module, requires no change to a standard NMT system. The only modification is in the training corpus, where they had introduced an artificial token at the beginning of each source sentence to denote the translation target language. \cite{Ha:2016aa} also showed that their universal encoder and decoder model is capable of zero-shot MT. The concept of translation between unseen language pairs is attractive, especially for low-resource language pairs, though these two models both underperformed than a pivot based system.

There are two reasons that could explain the gap between a zero-shot system and a pivot based system, language bias \parencite{Ha:2016aa, Ha:2017aa, Arivazhagan:2019aa} and poor generalization \parencite{Arivazhagan:2019aa}. Language bias means that during inference, the MT system tends to decode the target sentence into the wrong language, usually copying the source language or the bridging language \cite{Ha:2016aa}. It could be the consequence of always translating all source languages into the bridging language, hence make the model difficult to learn to translate the desired target language \parencite{Arivazhagan:2019aa}.

The other potential reason for the worse performance of a zero-shot system is poor generalization \parencite{Arivazhagan:2019aa}. When a zero-shot system is trained purely on the end-to-end translation objective, the model prefers to overfit the supervised translation direction features than learn more transferable language features.

To fix these two problems, there has been work on improving the preprocessing process \parencite{Lakew:2018aa}, parameter sharing \parencite{Firat:2016aa, Blackwood:2018aa}, additional loss penalty functions \parencite{Arivazhagan:2019aa} and pre-training modules using external information \parencite{Baziotis:2020aa}. In some cases, zero-shot system could achieve better performance than pivot based systems.

\subsection{MNMT Systems Based on Word Embeddings}

Like in Section \ref{sec:zero_shot_mt}, in cases where people need to translate from or into a low-resource language, they usually find it challenging to locate enough parallel data that consists of such kind of less common language. In addition to shared encoder and decoders between languages, if we could build up a vector space with word embeddings from different languages aligned, we could further leverage the similarity of word embeddings to compensate for the lack of parallel data \parencite{zou-etal-2013-bilingual}. We could find unseen words in the training data buy looking for their neighbors in the vector space.

There are successful applications of pre-trained word embeddings in a MT system, such as the embedding layer in an MT system \parencite{neishi-etal-2017-bag, Artetxe:2017aa}, the substitution of a supervised dictionary \parencite{Conneau:2017aa}, or an external supplementary extension \cite{inproceedings}. There are even cases where successfully trained a machine translation system using very little or none parallel data \parencite{Conneau:2017aa}. Nevertheless, in most MT systems, using pre-trained word embeddings purely as the embedding layer will not outperform other models such as Transformers \parencite{Vaswani:2017aa} and its other evolutions, largely because the training data for an MT system is usually several orders of magnitude larger than the monolingual pre-trained word embeddings. Typically pre-trained word embeddings are mainly introduced in MT systems dealing with low-resource languages.

For NMT systems focused on low resource language, \cite{Qi:2018aa} looked into the question of when and why are pre-trained word embeddings useful. They found that pre-trained word embeddings are consistently useful for all languages. The gains would be more visible if the source and target language are similar, such as languages within the same family. Also, pre-trained word embeddings work well only on MT systems with moderate performance. Pre-trained word embeddings can not work when there is not enough data to train a basic MT system. Finally, aligned word embeddings are useful in a multilingual MT system. For bilingual MT systems, pre-trained word embeddings do not necessarily need to be aligned.

Moreover, aligned word embeddings do not work well for morphologically rich languages such as Russian and Belarusian. \cite{Qi:2018aa} argues that this may be mainly due to the sparsity in the word embeddings files. Plus, most of the previous works target zero-shot language pairs, not on completely unseen languages. For language pairs $A \rightarrow \text{EN}$ and $\text{EN} \rightarrow B$, they are all interested in the unseen language pair $A \rightarrow B$. For language pairs that include an unseen language $C$, whether it is on the source side, or the target side, it remains to see how universal word embeddings could help translate in this scenario.

\chapter{Previous Work}
\label{chap:previous}


\chapter{Methodalogy}
\label{chap:method}

In this chapter, the author will perform experiments in a universal word embedding based MNMT system to see the transferability of the model on the test languages.

\section{Theoratical Feasibility}

As mentioned in Section \ref{sec:multilingual_word_embeddings}, \cite{Mikolov:2013ac} showed that there is a linear relationship between similar word embeddings in different languages. For each word pairs, assume their vector representations are $\{x_i, y_i\}_{i=1}^n$, we could calculate a transformation matrix $W$ such that $Wx_i$ approximates to $y_i$. In practice, people can learn $W$ by optimizing the following target function.

\begin{equation*}
  \min_W\sum_{i=1}^n||Wx_i-y_i||^2
\end{equation*}

\cite{Mikolov:2013ac} also showed their result in the word/phrase translation task for the approximated word embedding mappings. For some subsets of words, around 70\% of word embeddings match precisely with each other according to the P@5 score. If we relax the cosine similarity threshold to 0.6, the P@5 score would be as high as 90\%.

To convert words into vectors to be calculated in the neural network, NMT systems should treat each word as a word embedding. The value of these word embeddings could be learned directly during translation, but then the initialization is a crucial step since poor initialization could lead to slow converge or worse local minima \parencite{glorot2010understanding}. The situation could be even more challenging when translating with very few parallel corpora since there is no data to help the embedding layer converge to its ideal state. Hence the word embedding mapping technique above becomes appealing.

\cite{Qi:2018aa} explored how effective it is by using aligned pre-trained word embeddings in an NMT system. They found that regardless of languages, alignment is useful as long as it is applied in a multilingual setting. They believe that since both the source and the target side vector spaces are already aligned, the NMT system will learn how to transform a similar fashion from the source language to the target language.

Therefore, translating a completely unseen language is to test the transferability from known languages to an unknow language. It can be viewed as the question below --- Given a vector space $Z$ that consists of aligned word embeddings $\{a_i, b_i, c_i, ...\}$, how much does the NMT system knows about an unseen language $A$ if it was only trained on the remaining languages? In theory, since the word embeddings are clustered by their semantic meanings in the same vector space $Z$, we should be able to build loose mappings between the semantic centers from both the source and the target sides. The generalization ability of the system is the key to answer this question. Hence the author conducted some preliminary experiments below.

\section{Experiment Settings}
\label{sec:initial_exp_settings}

To get a basic multilingual MT system running, the author chose English (EN), German (De), and French (FR) to be the training languages. Let $C$ donate the final corpus, $l$ donates the language-specific corpus fragment and $Z$ is the set of corresponding candidate languages, the training language set is then $Z_{TRAIN} = {l_{EN}, l_{DE}, l_{FR}}$. For the test language set, the author picked up Swedish (SV), Hungarian (HU), and Hebrew (HE) being his test languages. Therefore $Z_{TEST} = {l_{SV}, l_{HU}, l_{HE}}$.

For each experiment, the author trained a basic MNMT system using a training corpus $C_{TEST}$ with all three training languages, including all six directions from the cartesian product without duplicates as below.

\begin{equation}
  C_{\text{TRAIN}} = \{x \times y \mid x, y \in Z_{\text{TRAIN}} \text{ and } x \neq y\}
\end{equation}

The equation below means that the MNMT system is tested on the test corpus with all three training languages and one of the test language. The test corpus consists of both translation directions of three different training language and that only test language.

\begin{equation}
  C_{\text{TEST}} = \{(x, y)\cup(y,x) \mid x \in Z_{\text{TRAIN}} \text{ and } y \in Z_{\text{TEST}}\}
\end{equation}

The author designed the experiments and picked up the training and target languages based on Language Similarity. \cite{Qi:2018aa} observed that pre-trained word embeddings are useful for languages from the same language family. The closer their relationship is, the higher the performance improvement is. Aligned word embeddings will also be beneficial if applied in an MNMT system consisting of languages from the same language family.

\subsection{Corpus and Preprocessing}

The author have used the TED talk subtitle corpus from \cite{Qi:2018aa} \footnote{\url{https://github.com/neulab/word-embeddings-for-nmt}} to train the MNMT. The whole corpus has roughly $\num{2.7e6}$ sentences split into three parts, train, dev, test at the ratio of $0.95:0.025:0.025$.

To build up the corpus for each experiment, the author has modified the original script from \cite{Qi:2018aa} and added a few customized features. In short, the script will extract shared sentences from each part of the split corpus to form up a common intersection used in training, developing, and testing. Since the experiments consist of languages that are relatively common in the TED project, this fine-tuned corpus is not too different from the original corpus, hence after all the sizes for the train, dev, and test split were kept.

For preprocessing, since the original TED corpus is already tokenized by Moses. Then the system \cite{Neubig:2018aa} turned all of the text into lower cases and applied a sentence length filter to remove any long sentences with more than 60 words. This sentence length filter prevents inferior performance in training. After that, when building the i2w and w2i index for the pre-trained embeddings, the author has also removed any words that are less frequent than two times to stop the system from overfitting by low-frequency words. All of the preprocess functions are built upon the built-in XNMT preprocess features \parencite{Neubig:2018aa}.

\subsection{Neural Network}

The neural network is a modified version of the one from \cite{Qi:2018aa}, which is built with XNMT \cite{Neubig:2018aa}. The only change is doubling the encoding layer to a 2-layer-bidirectional LSTM network, thus having more parameters to accommodate the additional information in a multilingual scenario. Everything else is the same as the original experiment settings, including the encoder-decoder model with attention \parencite{Bahdanau:2014aa} with a beam size of $5$, trained using batches of size $32$, dropout set to $0.1$, the Adam optimizer \parencite{Kingma:2014aa} and the evaluation metric BLEU score \parencite{papineni-etal-2002-bleu}. The initial learning starts at $0.0002$ and decays by $0.5$ when development BLEU score decreases \parencite{Denkowski:2017aa}.

\subsection{Embeddings}

As metioned in Section \ref{sec:fasttext}, the embeddings used in the experiments are fastText aligned word embeddings\footnote{\url{https://fasttext.cc/docs/en/aligned-vectors.html}}. They are based on the pre-trained vectors on Wikipedia\footnote{\url{https://www.wikipedia.org/}} using fastText \parencite{Bojanowski:2016aa}. The alignment is performed using RCSLS as in \cite{Joulin:2018aa}.

Each of the fastText word embedding file is language-specific and contains word embeddings in 300 dimensions. The author concatenated different language files to build up multilingual word embedding files for the MNMT system. If there is a shared word $w$ with two different vector values $\vec{v_a}$ and $\vec{v_b}$ in different embedding files, the average value of both vectors $\vec{v_{mean}}$ will be the new vector.

\begin{equation}
 \vec{v_{mean}} = (\vec{v_a} + \vec{v_b}) / 2
\end{equation}

In this way, there are possibilities that both of the unique semantic values in the two words $w_a$ and $w_b$ could be lost, as there are cases that word with distant meaning share the same spelling in different languages. However, people could also argue that many words with the same spelling do have a similar meaning. For example, the word \verb|café| means the same thing in English and French, as English borrowed that word from French. Later in the experiment, there will also be a different attempt where the system treats each word as a unique word even though they might share the same spelling. Both of the results will be available below.

\chapter{Results and Analysis}
\label{chap:results}

As anticipated, Swedish will get the best result among all three test languages. Its performance could even be on the same level as the baseline MNMT consists of only the training languages --- EN, DE, and FR. The other two test languages' performance will not be close to the Swedish one, and they could be less than 10 BLEU scores.

\begin{table}
  \centering
  \begin{tabular}{|r*{5}{|l}|}
    \hline
    Language & BLEU & 1gram & 2gram & 3gram & 4gram \\ [0.25ex]
    \hline\hline
    EN+DE+FR & 29.22 & 0.57 & 0.34 & 0.24 & 0.16 \\
    \hline
    SV & 1.12 & 0.16 & 0.02 & 0.00 & 0.00 \\ 
    \hline
    HU & 1.12 & 0.18 & 0.02 & 0.00 & 0.00 \\
    \hline
    HE & 1.02 & 0.16 & 0.02 & 0.00 & 0.00 \\
    \hline
  \end{tabular}
  \caption{Initial results for SV, HU and HE on the baseline system (Target language annotation only, dropout=0.3, trained on mixed language branch corpus.)}
  \label{table:initial_results}
\end{table}

In the results shown in Table \ref{table:initial_results}, Swedish, Hungarian and Hebrew all got unpredicted low BLEU scores. The expected high perfromance from Swedish did not appear in the experiment result. All of the three languages only achieved around 1 BLEU score. Also, since the system hardly translates any of the languages, it is hard to tell the relationship between language similarity and the model's performance. Nevertheless, the relatively low results on the test languages compared with the training languages indicate that cross-lingual embeddings are not rich enough for the model transfer in machine translation. However, when it comes to a random setting with no pre-trained embeddings, we see that the translation model trained with cross-lingual embeddings performs substantially better (Avg BLEU=1.2) than a model trained with random embeddings (BLEU=0.1).

In XNMT, one can also see the individual precision scores form 1 to 4 grams in the translation text. By looking at that details, all three languages had a significantly better unigram precision score than their bigram, trigram, and quadgram precision score. The bigram precision score in Swedish was about half of its unigram scores. The precison score on trigrams and quadgrams are close to 0 on all languages, which again is a sign showing the MNMT system has little transferability from known training languages to an unknown test language.

To increase the transferability from known languages to unknown languages, the author has tried various techniques, such as increasing dropout rate. We have observed small improvements (average 0.5 BLEU score increase), but since this technique improves the zero-shot performance at the cost of supervised translation directions \cite{Arivazhagan:2019aa}, we decided to explore other approaches in below.

\section{The Effect of Source/Target Language Annotation}

Previous initial results opened up some follow up experiments to see what could be improved. The first improvement is to alter the way the target language annotation in the source sentences, inspired by \cite{Blackwood:2018aa}. In the original corpus building script, it will add a custom \verb|__{lang_id}__| token at the front of each source sentence, as suggested by \cite{Johnson:2016aa}. A sentence in the annotated source text whose target language is German would look like 

\begin{verbatim}
  __de__ and we struggle with how to deal with them .
\end{verbatim}

Later two other tokens --- a single source token and a source token together with a target token, were added into the experiments. Hence a sentence in English would look like this. 

\begin{verbatim}
  __en__ and we struggle with how to deal with them .
\end{verbatim}

When it needs to be translated into German, the annotation would then become 

\begin{verbatim}
  __en__ __de__ and we struggle with how to deal with them .
\end{verbatim}

\begin{table}
  \centering
  \begin{tabular}{|r*{3}{|l}|}
  \hline
  Languages & TGT & SRC & Full \\
  \hline\hline
  EN+DE+FR & 29.22 & 17.59 & 28.73 \\
  \hline
  SV & 1.12 & 0.00 & 1.16 \\
  \hline
  HU & 1.12 & 0.00 & 1.12 \\
  \hline
  HE & 1.02 & 0.00 & 1.02 \\
  \hline
  \end{tabular}
  \caption{BLEU scores for different language annotations (Target only, source only and full annotation)}
  \label{table:altering_lang_id}
\end{table}

The results are in Table \ref{table:altering_lang_id}. Adding a source token together with the target token did not change the overall result much, but removing the target token had a negative impact on the final BLEU score. This discovery is in line with previous claims and results \parencite{Johnson:2016aa, Blackwood:2018aa}, as the MNMT system requires a target token at the beginning of each sentence to help identify the target language. The difference between different langauge annotations indicated that a word embedding based MNMT system would also automatically learn the source language during training. People should only annotate the target language into the source text.

\section{The Effect of Language Similarity}

Previous hypothesis believes that Swedish will perform better the Hungarian and Hebrew for its closer relationship to the training languages. To deeper undetstand the question, the author have further designed experiments to see if language similarity will improve the results.

The additional experiments will still use Swedish as the test langauge while remove French as the training language to homogenize the training language set more towards Swedish. In the previous training language set, French is the only training language that is Romanic. By replacing French with Danish, all of the training languages are now Germanic, as well as the test language Swedish. The author has also included two more Germanic languages as the training language, Dutch (NL) and Norwegian (NO). The author began with experiment trained on English, German and Danish, and added the additional training languages one by one in the next two expriments. Everything else is the same. The results are shown in Table \ref{table:language_similarity}.

\begin{table}
  \centering
  \begin{tabular}{|r*{5}{|l}|}
  \hline
  Language & BLEU & 1gram & 2gram & 3gram & 4gram \\ [0.25ex]
  \hline\hline
  EN+DE+FR & 1.12 & 0.16 & 0.02 & 0.00 & 0.00 \\
  \hline
  EN+DE+DA & 4.1 & 0.28 & 0.07 & 0.02 & 0.01 \\
  \hline
  +NL & 3.3 & 0.23 & 0.05 & 0.02 & 0.00 \\ 
  \hline
  +NO & 4.7 & 0.31 & 0.07 & 0.03 & 0.01 \\
  \hline
  \end{tabular}
  \caption{Results for langauge similarity tested on the Swedish language. Three other Germanic languages DA, NL and NO were added one by one into the training corpus.}
  \label{table:language_similarity}
\end{table}

As the results shows, the system gained most improvements when Danish and Norwegian were added. Desipte Dutch and Swedish are both Germanic languages, Dutch does not help the MNMT system to learn how to translate from Swedish or into Swedish a lot. This confirms that close languages would benefit each other more than distant languages in a MNMT systemn using pre-trained word embeddings \parencite{Qi:2018aa}.

Swedish, Danish and Norwegian have deep historical relations to each other. As a result, these languages share many vocabularies as well as grammar and syntax ruls. To study how much of such kind of benefit were brought by shared vocabluaries or their similar syntax, each word in the training corpus was tagged by its source language token to distinguish its origins. Punctuations are not distinguished among languages, which means they don't receive a language-specific token. The word embeddings also tagged to point it to the correct source word. A Swedish sentence that needs to be translated into German is then 

\begin{verbatim}
  __de__ <<sv>>och <<sv>>vi <<sv>>kämpar <<sv>>med <<sv>>dem .
\end{verbatim}

The assumption behind the word origin token is that, if the result suffers when each word differs by its origin language, the MNMT system would primarily translate by shared vocabularies between language; if its results still holds after the modificaiton, it would primarily learn translation from shared syntax information instead.

The system has obtained 1.7 BLEU score on the EN+DE+FR to SV experiment. It showed that if each word is no longer allowed to be shared between languages the models's performance would dramatically decrease, hence most of the improvements were brought by the fact that Swedish, Danish and Norwegian have a large amount of common vocabluaries. On the other side, it also indicates that the system didn't learn too much syntactic information during training. Even though these languages have similar grammar structures, the system didn't catch it very well, otherwise we would see smaller BLEU score gap between the results as the close grammar relationship will be preserved in the embedding layer. The MNMT system here primarily learns lexicon translation.

\section{The Effect of the Transformed Vector Space}

In addition to the low language similarity between the training languages and the test languages (except in the case of Swedish), we also hypothesis that the poor transferability is due to transformed vector space in the translation model. We believe that after a series of linear opeartion from the neural network onto the word embeddings, the output vector space is no longer aligned with the input vector space.

The whole experiment is based on the hypothesis that our NMT system have already learned the genrally mapping between words in the source vector space and the ones in the target vector space, even though the correct word in the target word space hasn't been seen by the system during training. However since every aligned word embeddings are grouped by their semantics, the correct target word should also be around the wrong output word.

By looking closely from the predicted translation in Table \ref{table:initial_results}, we have observed the contrary --- Almost non of the words in the output text got translated to the correct word in the desired languages. They were either translated into one of the training languages, or were entirely copied directly from the source text, see Appendix \ref{chap:example_output}. The BLEU score gains were from punctuations and a small collection of words shared between languages (e.g. property nouns).

Taking a step further, when analyze both translation directions there are other traces to support our transformed vector space hypothesis. We have conducted comparisions on both directions on the Swedish language experiment, as shown in Table \ref{table:directional_results}. When tranlsating from SV to the combined EN+DE+DA text, we could achieve almost 6 BLEU scores, which is much better than the nearly zero score when translating from the other direction. Also compared to the combined precision scores on the same experiment from Table \ref{table:language_similarity}, the results of the translation direction EN+DE+DA to SV contributed almost nothing to the combined translation performance.

\begin{table}
  \centering
  \begin{tabular}{|r*{5}{|l}|}
  \hline
  Language & BLEU & 1gram & 2gram & 3gram & 4gram \\ [0.25ex]
  \hline\hline
  EN+DE+DA $\rightarrow$ SV & 0.65 & 0.14 & 0.01 & 0.00 & 0.00 \\
  \hline
  SV $\rightarrow$ EN+DE+DA & 6.00 & 0.33 & 0.08 & 0.03 & 0.01 \\
  \hline
  \end{tabular}
  \caption{Results for individual translation direction between EN+DE+DA and SV.}
  \label{table:directional_results}
\end{table}

Thus, it is highly possible to suspect the output vectors from the model's decoder have been altered and are no longer in the same vector space as the input word embeddings. In this case, the transformed vector space has also made less sense to search for the correct word vector neighbours close to the predicted ouput vector in the output vector space. However, there opens a new possible to translate from completely unknown languages to known languages. We could perform a lexicon replacement based on the Euclidean distance between words in the unknown language and one of the known languages, then feed the processed text into a translation model which has already been trained on known languages. During the whole process, the unknown langauge remains untouched by the translation model hence it still qualifies as zero-resource translation. We will discuss the lexicon replacement process below.

\subsection{Lexicon Replacement By Euclidean Distance}

Suppose we now have a vector space $S$ that contains aligned word embeddings in the unknown language and the known languages, respectively. We donate them as $W_x$ and $W_k$ For each $w_x\in W_x$ there exists at least one mapping to a target word in the known languages $w_k \in W_k$. We are looking for that specific $w_k$ that are with in a specific radius of the original $w_s$. The distance should still be relatively small so that $w_s$ and $w_k$ are both considered to be a effective translation of each other.

In theory to determine the nearyby neighbour $w_k$ we can use different kinds of metrics. Here the author have chosen to use the Euclidean distance where determines the distance between $w_s$ and $w_k$ as

\begin{equation}
  d(w_s, w_k)=\sqrt{\sum_{i=1}^n{(w_{s_i}-w_{k_i})}^2}
\end{equation}

The distance $d$ is a variable here and its value needs to be determined as well. Hence there are experiments to test the distance argument $d$ by different experiments, ranging from $d=0.25$ to $d=5$.

The algorithm is described in Algorithm \ref{algo:subsitution}

\begin{algorithm}[H]
  \SetAlgoLined
  \KwIn{hypothesis $H$, source language embeddings $E_s$, target language embeddings $E_t$, distance threshold $D$}
  \KwResult{Updated hypothesis $H^\prime$ with words being replaced by their neighbors in the desired language}

  Build kd-tree $T$ from $E_s$
  \For(each line $l$ in the source hypothesis $H$){$l \in H$}{
    \For(each word $w$ in line $l$){$w \in l$}{
      \uIf{$w$ is a punctuation}{
        skip $w$\;
      }
      \uElseIf{$w$ is an unknown word}{
        skip $w$\;
      }
      \Else{
        query distance $d(w, w^\prime)$ for $w$ in $T$\;
        \If{$d<D$}{
          replace $w$ with the correspoding $w^\prime$
        }
      }
    }
  }
  \caption{Pesudo code for output hypothesis word subsitution. Each word in the NMT output hypothesis that are not in the desired language will be replaced by its cloeset neighbour in that language.}
  \label{algo:subsitution}
\end{algorithm}

Performing a distance query on a vector space that has more than $\num{3e6}$ vectors is slow, especially when all these vectors are considered to be high dimensional vectors. The code was implemented with SciPy \parencite{Virtanen:2019aa}. There are algothrims like KD-tree \parencite{Maneewongvatana:aa} that could reduce the calcualtion time for low-dimensional vectors, but for vectors that are higher than 20 dimentions it is not necessarily faster than brutal force.\footnote{As descibed on the API document, "High-dimensional nearest-neighbor queries are a substantial open problem in computer science.", \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html}} On the other hand, based on the Johnson–Lindenstrauss theorem \parencite{johnson1984extensions}, a vector space should have at least more than 300 dimensions to distinguish $\num{1e6}$ vectors in it. As the aligned vector space in fastText contains more than $\num{3e6}$ words, the dimensions could not be compressed any more or you are at risk of not be able to distinguish each word. All in all, the script is slow at subsitute every word in the output hypothesis into the correspoding one in the desired language.

We have performed the lexicon replacement experiment on a SV+NO text from the same TED text corpus \parencite{Qi:2018aa}, feeded into a trained tranlsation model on the NO $\leftrightarrow$ EN+DE+DA corpus using NO as the pivot language. The BLEU scores are shown in Figure \ref{fig:target_replacement}. For comparison, we have also performed Algorithm \ref{algo:subsitution} on the output vector space to demostate the difference of applying the same algorithm on the input and the output vector space.

\begin{figure}
  \centering
  \resizebox{0.7\columnwidth}{!}{%
      \begin{tikzpicture}
        \begin{axis}[
          xlabel=d,
          ylabel=BLEU]
        \addplot[color=red,mark=x] coordinates {
          (0,4.05)
          (0.25,4.0)
          (0.5,4.0)
          (1,4.1)
          (2,4.1)
          (3,4.1)
          (4,4.1)
        };
        \end{axis}
      \end{tikzpicture}
  }
  \caption{BLEU scores for the lexicon replacement algorithm applied on both the input and the output vector space.}
  \label{fig:target_replacement}
\end{figure}

\chapter{Conclusion and Future Work}
\label{chap:conclusion}

From the observation in the last section, we hypothesis that a large amount of error in the output is because the output layer of the model's decoder does not provide any mechanism to transfer the translation knowledge between languages. The layer has one entry per each word in the entire vocabulary set. The entries are activated independently, one at each time. Since the model does not see any example from the unseen test languages during the training phase, the output connections corresponding to the unseen words are down-weighted during the training phase. Hence, it is improbable for the model to output words from the unseen languages, except for those shared with the training languages.

\appendix
\chapter{Example Output from the MNMT Model}
\label{chap:example_output}

This sample output is taken from the expriment described in Section \ref{sec:initial_exp_settings}. Its perfromance result is in Table \ref{table:initial_results}.



\printbibliography
\end{document}
