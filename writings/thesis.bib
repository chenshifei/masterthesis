%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Shifei Chen at 2020-07-29 22:59:04 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{glorot2010understanding,
	Author = {Glorot, Xavier and Bengio, Yoshua},
	Booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	Date-Added = {2020-07-26 01:48:27 +0200},
	Date-Modified = {2020-07-26 01:48:27 +0200},
	Pages = {249--256},
	Title = {Understanding the difficulty of training deep feedforward neural networks},
	Year = {2010}}

@article{Heinzerling:2019aa,
	Abstract = {Pretrained contextual and non-contextual subword embeddings have become available in over 250 languages, allowing massively multilingual NLP. However, while there is no dearth of pretrained embeddings, the distinct lack of systematic evaluations makes it difficult for practitioners to choose between them. In this work, we conduct an extensive evaluation comparing non-contextual subword embeddings, namely FastText and BPEmb, and a contextual representation method, namely BERT, on multilingual named entity recognition and part-of-speech tagging. We find that overall, a combination of BERT, BPEmb, and character representations works best across languages and tasks. A more detailed analysis reveals different strengths and weaknesses: Multilingual BERT performs well in medium- to high-resource languages, but is outperformed by non-contextual subword embeddings in a low-resource setting.},
	Author = {Benjamin Heinzerling and Michael Strube},
	Date-Added = {2020-07-23 02:21:43 +0200},
	Date-Modified = {2020-07-23 02:21:43 +0200},
	Eprint = {1906.01569},
	Month = {06},
	Title = {Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation},
	Url = {https://arxiv.org/pdf/1906.01569.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTA2LjAxNTY5LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTA2LjAxNTY5LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkwNi4wMTU2OS5wZGYADgAeAA4AMQA5ADAANgAuADAAMQA1ADYAOQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MDYuMDE1NjkucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1906.01569}}

@article{Vaswani:2017aa,
	Abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	Author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	Date-Added = {2020-07-19 00:06:29 +0200},
	Date-Modified = {2020-07-19 00:06:29 +0200},
	Eprint = {1706.03762},
	Month = {06},
	Title = {Attention Is All You Need},
	Url = {https://arxiv.org/pdf/1706.03762.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzA2LjAzNzYyLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzA2LjAzNzYyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcwNi4wMzc2Mi5wZGYADgAeAA4AMQA3ADAANgAuADAAMwA3ADYAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MDYuMDM3NjIucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1706.03762}}

@inproceedings{inproceedings,
	Author = {Di Gangi, Mattia and Federico, Marcello},
	Date-Added = {2020-07-17 01:28:20 +0200},
	Date-Modified = {2020-07-17 01:28:20 +0200},
	Month = {12},
	Title = {Monolingual Embeddings for Low Resourced Neural Machine Translation},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA4Li4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9tb25vbGluZ3VhbC1lbWJlZGRpbmdzLWxvdy5wZGZPEQGSAAAAAAGSAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8ebW9ub2xpbmd1YWwtZW1iZWRkaW5ncy1sb3cucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAD4vOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOm1vbm9saW5ndWFsLWVtYmVkZGluZ3MtbG93LnBkZgAOAD4AHgBtAG8AbgBvAGwAaQBuAGcAdQBhAGwALQBlAG0AYgBlAGQAZABpAG4AZwBzAC0AbABvAHcALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASADxVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9tb25vbGluZ3VhbC1lbWJlZGRpbmdzLWxvdy5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQAXwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAH1},
	Bdsk-Url-1 = {https://www.researchgate.net/publication/321706277_Monolingual_Embeddings_for_Low_Resourced_Neural_Machine_Translation}}

@article{Artetxe:2017aa,
	Abstract = {In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.},
	Author = {Mikel Artetxe and Gorka Labaka and Eneko Agirre and Kyunghyun Cho},
	Date-Added = {2020-07-17 01:24:59 +0200},
	Date-Modified = {2020-07-17 01:24:59 +0200},
	Eprint = {1710.11041},
	Month = {10},
	Title = {Unsupervised Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1710.11041.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzEwLjExMDQxLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzEwLjExMDQxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcxMC4xMTA0MS5wZGYADgAeAA4AMQA3ADEAMAAuADEAMQAwADQAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MTAuMTEwNDEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1710.11041}}

@inproceedings{neishi-etal-2017-bag,
	Abstract = {In this paper, we describe the team UT-IIS{'}s system and results for the WAT 2017 translation tasks. We further investigated several tricks including a novel technique for initializing embedding layers using only the parallel corpus, which increased the BLEU score by 1.28, found a practical large batch size of 256, and gained insights regarding hyperparameter settings. Ultimately, our system obtained a better result than the state-of-the-art system of WAT 2016. Our code is available on \url{https://github.com/nem6ishi/wat17}.},
	Address = {Taipei, Taiwan},
	Author = {Neishi, Masato and Sakuma, Jin and Tohda, Satoshi and Ishiwatari, Shonosuke and Yoshinaga, Naoki and Toyoda, Masashi},
	Booktitle = {Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017)},
	Date-Added = {2020-07-17 01:23:26 +0200},
	Date-Modified = {2020-07-17 01:23:26 +0200},
	Month = nov,
	Pages = {99--109},
	Publisher = {Asian Federation of Natural Language Processing},
	Title = {A Bag of Useful Tricks for Practical Neural Machine Translation: Embedding Layer Initialization and Large Batch Size},
	Url = {https://www.aclweb.org/anthology/W17-5708.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy93MTctNTcwOC5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MdzE3LTU3MDgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOncxNy01NzA4LnBkZgAOABoADAB3ADEANwAtADUANwAwADgALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy93MTctNTcwOC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W17-5708}}

@article{Blackwood:2018aa,
	Abstract = {Multilingual machine translation addresses the task of translating between multiple source and target languages. We propose task-specific attention models, a simple but effective technique for improving the quality of sequence-to-sequence neural multilingual translation. Our approach seeks to retain as much of the parameter sharing generalization of NMT models as possible, while still allowing for language-specific specialization of the attention model to a particular language-pair or task. Our experiments on four languages of the Europarl corpus show that using a target-specific model of attention provides consistent gains in translation quality for all possible translation directions, compared to a model in which all parameters are shared. We observe improved translation quality even in the (extreme) low-resource zero-shot translation directions for which the model never saw explicitly paired parallel data.},
	Author = {Graeme Blackwood and Miguel Ballesteros and Todd Ward},
	Date-Added = {2020-07-17 00:33:21 +0200},
	Date-Modified = {2020-07-17 00:33:21 +0200},
	Eprint = {1806.03280},
	Month = {06},
	Title = {Multilingual Neural Machine Translation with Task-Specific Attention},
	Url = {https://arxiv.org/pdf/1806.03280.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODA2LjAzMjgwLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODA2LjAzMjgwLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgwNi4wMzI4MC5wZGYADgAeAA4AMQA4ADAANgAuADAAMwAyADgAMAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MDYuMDMyODAucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1806.03280}}

@article{Firat:2016aa,
	Abstract = {In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.},
	Author = {Orhan Firat and Baskaran Sankaran and Yaser Al-Onaizan and Fatos T. Yarman Vural and Kyunghyun Cho},
	Date-Added = {2020-07-17 00:32:21 +0200},
	Date-Modified = {2020-07-17 00:32:21 +0200},
	Eprint = {1606.04164},
	Month = {06},
	Title = {Zero-Resource Translation with Multi-Lingual Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1606.04164.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA2LjA0MTY0LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA2LjA0MTY0LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNi4wNDE2NC5wZGYADgAeAA4AMQA2ADAANgAuADAANAAxADYANAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDYuMDQxNjQucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.04164}}

@article{Lakew:2018ab,
	Abstract = {Recently, neural machine translation (NMT) has been extended to multilinguality, that is to handle more than one translation direction with a single system. Multilingual NMT showed competitive performance against pure bilingual systems. Notably, in low-resource settings, it proved to work effectively and efficiently, thanks to shared representation space that is forced across languages and induces a sort of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot inference across language pairs never seen at training time. Despite the increasing interest in this framework, an in-depth analysis of what a multilingual NMT model is capable of and what it is not is still missing. Motivated by this, our work (i) provides a quantitative and comparative analysis of the translations produced by bilingual, multilingual and zero-shot systems; (ii) investigates the translation quality of two of the currently dominant neural architectures in MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively explores how the closeness between languages influences the zero-shot translation. Our analysis leverages multiple professional post-edits of automatic translations by several different systems and focuses both on automatic standard metrics (BLEU and TER) and on widely used error categories, which are lexical, morphology, and word order errors.},
	Author = {Surafel M. Lakew and Mauro Cettolo and Marcello Federico},
	Date-Added = {2020-07-17 00:05:54 +0200},
	Date-Modified = {2020-07-17 00:05:54 +0200},
	Eprint = {1806.06957},
	Month = {06},
	Title = {A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1806.06957.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODA2LjA2OTU3LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODA2LjA2OTU3LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgwNi4wNjk1Ny5wZGYADgAeAA4AMQA4ADAANgAuADAANgA5ADUANwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MDYuMDY5NTcucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1806.06957}}

@article{Ha:2016aa,
	Abstract = {In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.},
	Author = {Thanh-Le Ha and Jan Niehues and Alexander Waibel},
	Date-Added = {2020-07-16 00:09:28 +0200},
	Date-Modified = {2020-07-16 00:09:28 +0200},
	Eprint = {1611.04798},
	Month = {11},
	Title = {Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder},
	Url = {https://arxiv.org/pdf/1611.04798.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjExLjA0Nzk4LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjExLjA0Nzk4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYxMS4wNDc5OC5wZGYADgAeAA4AMQA2ADEAMQAuADAANAA3ADkAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MTEuMDQ3OTgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1611.04798}}

@article{Lakew:2019aa,
	Abstract = {Multilingual Neural Machine Translation (MNMT) for low-resource languages (LRL) can be enhanced by the presence of related high-resource languages (HRL), but the relatedness of HRL usually relies on predefined linguistic assumptions about language similarity. Recently, adapting MNMT to a LRL has shown to greatly improve performance. In this work, we explore the problem of adapting an MNMT model to an unseen LRL using data selection and model adaptation. In order to improve NMT for LRL, we employ perplexity to select HRL data that are most similar to the LRL on the basis of language distance. We extensively explore data selection in popular multilingual NMT settings, namely in (zero-shot) translation, and in adaptation from a multilingual pre-trained model, for both directions (LRL-en). We further show that dynamic adaptation of the model's vocabulary results in a more favourable segmentation for the LRL in comparison with direct adaptation. Experiments show reductions in training time and significant performance gains over LRL baselines, even with zero LRL data (+13.0 BLEU), up to +17.0 BLEU for pre-trained multilingual model dynamic adaptation with related data selection. Our method outperforms current approaches, such as massively multilingual models and data augmentation, on four LRL.},
	Author = {Surafel M. Lakew and Alina Karakanta and Marcello Federico and Matteo Negri and Marco Turchi},
	Date-Added = {2020-07-11 23:34:12 +0200},
	Date-Modified = {2020-07-11 23:34:12 +0200},
	Eprint = {1910.13998},
	Month = {10},
	Title = {Adapting Multilingual Neural Machine Translation to Unseen Languages},
	Url = {https://arxiv.org/pdf/1910.13998.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTEwLjEzOTk4LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTEwLjEzOTk4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkxMC4xMzk5OC5wZGYADgAeAA4AMQA5ADEAMAAuADEAMwA5ADkAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MTAuMTM5OTgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1910.13998}}

@article{Tan:2019aa,
	Abstract = {Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods},
	Author = {Xu Tan and Jiale Chen and Di He and Yingce Xia and Tao Qin and Tie-Yan Liu},
	Date-Added = {2020-07-11 23:30:15 +0200},
	Date-Modified = {2020-07-11 23:30:15 +0200},
	Eprint = {1908.09324},
	Month = {08},
	Title = {Multilingual Neural Machine Translation with Language Clustering},
	Url = {https://arxiv.org/pdf/1908.09324.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTA4LjA5MzI0LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTA4LjA5MzI0LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkwOC4wOTMyNC5wZGYADgAeAA4AMQA5ADAAOAAuADAAOQAzADIANAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MDguMDkzMjQucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1908.09324}}

@article{Bjerva_2019,
	Author = {Bjerva, Johannes and {\"O}stling, Robert and Veiga, Maria Han and Tiedemann, J{\"o}rg and Augenstein, Isabelle},
	Date-Added = {2020-07-11 23:29:37 +0200},
	Date-Modified = {2020-07-11 23:29:37 +0200},
	Doi = {10.1162/coli_a_00351},
	Issn = {1530-9312},
	Journal = {Computational Linguistics},
	Month = {Jun},
	Number = {2},
	Pages = {381--389},
	Publisher = {MIT Press - Journals},
	Title = {What Do Language Representations Really Represent?},
	Url = {http://dx.doi.org/10.1162/coli_a_00351},
	Volume = {45},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/coli_a_00351}}

@mastersthesis{Moss1437826,
	Abstract = {In this work, we test two novel methods of using word embeddings to detect lexical semantic change, attempting to overcome limitations associated with conventional approaches to this problem. Using a diachronic corpus spanning over a hundred years, we generate word embeddings for each decade with the intention of evaluating how meaning changes are represented in embeddings for the same word across time. Our approach differs from previous works in this field in that we encode words as probabilistic Gaussian distributions and bimodal probabilistic Gaussian mixtures, rather than conventional word vectors. We provide a discussion and analysis of our results, comparing the approaches we implemented with those used in previous works. We also conducted further analysis on whether additional information regarding the nature of semantic change could be discerned from particular qualities of the embeddings we generated for our experiments. In our results, we find that encoding words as probabilistic Gaussian embeddings can provide an enhanced degree of reliability with regard to detecting lexical semantic change. Furthermore, we are able to represent additional information regarding the nature of such changes through the variance of these embeddings. Encoding words as bimodal Gaussian mixtures however is generally unsuccessful for this task, proving to be not reliable enough at distinguishing between discrete senses to effectively detect and measure such changes. We provide potential explanations for the results we observe, and propose improvements that can be made to our approach to potentially improve performance. },
	Author = {Moss, Adam},
	Date-Added = {2020-07-11 23:05:56 +0200},
	Date-Modified = {2020-07-11 23:05:56 +0200},
	Institution = {Uppsala University, Department of Linguistics and Philology},
	Keywords = {historical linguistics, historical semantics, lexical semantic change, diachronic semantic change, word embeddings, probabilistic word embeddings, gaussian word embeddings},
	Pages = {45},
	School = {Uppsala University, Department of Linguistics and Philology},
	Title = {Detecting Lexical Semantic Change Using Probabilistic Gaussian Word Embeddings},
	Year = {2020},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxApLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9mdWxsdGV4dDAxMS5wZGZPEQFYAAAAAAFYAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8PZnVsbHRleHQwMTEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAC8vOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmZ1bGx0ZXh0MDExLnBkZgAADgAgAA8AZgB1AGwAbAB0AGUAeAB0ADAAMQAxAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgAtVXNlcnMvc2hpZmVpL0RvY3VtZW50cy9QYXBlcnMvZnVsbHRleHQwMTEucGRmAAATAAEvAAAVAAIADf//AAAACAANABoAJABQAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAaw=}}

@article{Lakew:2018aa,
	Abstract = {Recent work on multilingual neural machine translation reported competitive performance with respect to bilingual models and surprisingly good performance even on (zeroshot) translation directions not observed at training time. We investigate here a zero-shot translation in a particularly lowresource multilingual setting. We propose a simple iterative training procedure that leverages a duality of translations directly generated by the system for the zero-shot directions. The translations produced by the system (sub-optimal since they contain mixed language from the shared vocabulary), are then used together with the original parallel data to feed and iteratively re-train the multilingual network. Over time, this allows the system to learn from its own generated and increasingly better output. Our approach shows to be effective in improving the two zero-shot directions of our multilingual model. In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models. Further analysis shows that there is also a slight improvement in the non-zero-shot language directions.},
	Author = {Surafel M. Lakew and Quintino F. Lotito and Matteo Negri and Marco Turchi and Marcello Federico},
	Date-Added = {2020-07-11 22:58:06 +0200},
	Date-Modified = {2020-07-11 22:58:06 +0200},
	Eprint = {1811.01389},
	Month = {11},
	Title = {Improving Zero-Shot Translation of Low-Resource Languages},
	Url = {https://arxiv.org/pdf/1811.01389.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODExLjAxMzg5LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODExLjAxMzg5LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgxMS4wMTM4OS5wZGYADgAeAA4AMQA4ADEAMQAuADAAMQAzADgAOQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MTEuMDEzODkucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1811.01389}}

@article{Ha:2017aa,
	Abstract = {In this paper, we proposed two strategies which can be applied to a multilingual neural machine translation system in order to better tackle zero-shot scenarios despite not having any parallel corpus. The experiments show that they are effective in terms of both performance and computing resources, especially in multilingual translation of unbalanced data in real zero-resourced condition when they alleviate the language bias problem.},
	Author = {Thanh-Le Ha and Jan Niehues and Alexander Waibel},
	Date-Added = {2020-07-11 22:57:52 +0200},
	Date-Modified = {2020-07-11 22:57:52 +0200},
	Eprint = {1711.07893},
	Month = {11},
	Title = {Effective Strategies in Zero-Shot Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1711.07893.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzExLjA3ODkzLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzExLjA3ODkzLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcxMS4wNzg5My5wZGYADgAeAA4AMQA3ADEAMQAuADAANwA4ADkAMwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MTEuMDc4OTMucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1711.07893}}

@article{Arivazhagan:2019aa,
	Abstract = {Multilingual Neural Machine Translation (NMT) models are capable of translating between multiple source and target languages. Despite various approaches to train such models, they have difficulty with zero-shot translation: translating between language pairs that were not together seen during training. In this paper we first diagnose why state-of-the-art multilingual NMT models that rely purely on parameter sharing, fail to generalize to unseen language pairs. We then propose auxiliary losses on the NMT encoder that impose representational invariance across languages. Our simple approach vastly improves zero-shot translation quality without regressing on supervised directions. For the first time, on WMT14 English-FrenchGerman, we achieve zero-shot performance that is on par with pivoting. We also demonstrate the easy scalability of our approach to multiple languages on the IWSLT 2017 shared task.},
	Author = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Roee Aharoni and Melvin Johnson and Wolfgang Macherey},
	Date-Added = {2020-07-11 22:57:41 +0200},
	Date-Modified = {2020-07-11 22:57:41 +0200},
	Eprint = {1903.07091},
	Month = {03},
	Title = {The Missing Ingredient in Zero-Shot Neural Machine Translation},
	Url = {https://arxiv.org/pdf/1903.07091.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTAzLjA3MDkxLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTAzLjA3MDkxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkwMy4wNzA5MS5wZGYADgAeAA4AMQA5ADAAMwAuADAANwAwADkAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MDMuMDcwOTEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1903.07091}}

@article{Tsvetkov:2016aa,
	Abstract = {We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually.},
	Author = {Yulia Tsvetkov and Sunayana Sitaram and Manaal Faruqui and Guillaume Lample and Patrick Littell and David Mortensen and Alan W Black and Lori Levin and Chris Dyer},
	Date-Added = {2020-07-06 23:26:36 +0200},
	Date-Modified = {2020-07-06 23:29:52 +0200},
	Eprint = {1605.03832},
	Month = {05},
	Title = {Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning},
	Url = {https://arxiv.org/pdf/1605.03832.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA1LjAzODMyLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA1LjAzODMyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNS4wMzgzMi5wZGYADgAeAA4AMQA2ADAANQAuADAAMwA4ADMAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDUuMDM4MzIucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1605.03832}}

@inproceedings{malaviya-etal-2017-learning,
	Abstract = {One central mystery of neural NLP is what neural models {``}know{''} about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages? Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one NMT system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages geographic and phylogenetic neighbors.},
	Address = {Copenhagen, Denmark},
	Author = {Malaviya, Chaitanya and Neubig, Graham and Littell, Patrick},
	Booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2020-06-23 00:07:04 +0200},
	Date-Modified = {2020-06-23 00:07:04 +0200},
	Doi = {10.18653/v1/D17-1268},
	Month = sep,
	Pages = {2529--2535},
	Publisher = {Association for Computational Linguistics},
	Title = {Learning Language Representations for Typology Prediction},
	Url = {https://www.aclweb.org/anthology/D17-1268.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9kMTctMTI2OC5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MZDE3LTEyNjgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmQxNy0xMjY4LnBkZgAOABoADABkADEANwAtADEAMgA2ADgALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9kMTctMTI2OC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D17-1268}}

@inproceedings{littell-etal-2017-uriel,
	Abstract = {We introduce the URIEL knowledge base for massively multilingual NLP and the lang2vec utility, which provides information-rich vector identifications of languages drawn from typological, geographical, and phylogenetic databases and normalized to have straightforward and consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors.},
	Address = {Valencia, Spain},
	Author = {Littell, Patrick and Mortensen, David R. and Lin, Ke and Kairis, Katherine and Turner, Carlisle and Levin, Lori},
	Booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	Date-Added = {2020-06-23 00:02:01 +0200},
	Date-Modified = {2020-06-23 00:04:32 +0200},
	Month = apr,
	Pages = {8--14},
	Publisher = {Association for Computational Linguistics},
	Read = {0},
	Title = {{URIEL} and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors},
	Url = {https://www.aclweb.org/anthology/E17-2002.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9lMTctMjAwMi5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MZTE3LTIwMDIucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmUxNy0yMDAyLnBkZgAOABoADABlADEANwAtADIAMAAwADIALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9lMTctMjAwMi5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/E17-2002}}

@article{Smith:2017aa,
	Abstract = {Usually bilingual word vectors are trained "online". Mikolov et al. showed they can also be found "offline", whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel "inverted softmax" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34\% to 43\%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a "pseudo-dictionary" from the identical character strings which appear in both languages, achieving 40\% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68\%.},
	Author = {Samuel L. Smith and David H. P. Turban and Steven Hamblin and Nils Y. Hammerla},
	Date-Added = {2020-06-18 17:03:21 +0200},
	Date-Modified = {2020-06-18 17:10:12 +0200},
	Eprint = {1702.03859},
	Month = {02},
	Title = {Offline bilingual word vectors, orthogonal transformations and the inverted softmax},
	Url = {https://arxiv.org/pdf/1702.03859.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzAyLjAzODU5LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzAyLjAzODU5LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcwMi4wMzg1OS5wZGYADgAeAA4AMQA3ADAAMgAuADAAMwA4ADUAOQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MDIuMDM4NTkucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1702.03859}}

@article{Johnson:2016aa,
	Abstract = {We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\rightarrow$French and surpasses state-of-the-art results for English$\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\rightarrow$English and German$\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
	Author = {Melvin Johnson and Mike Schuster and Quoc V. Le and Maxim Krikun and Yonghui Wu and Zhifeng Chen and Nikhil Thorat and Fernanda Vi{\'e}gas and Martin Wattenberg and Greg Corrado and Macduff Hughes and Jeffrey Dean},
	Date-Added = {2020-06-13 09:55:18 +0200},
	Date-Modified = {2020-06-13 09:55:18 +0200},
	Eprint = {1611.04558},
	Month = {11},
	Title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	Url = {https://arxiv.org/pdf/1611.04558.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjExLjA0NTU4LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjExLjA0NTU4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYxMS4wNDU1OC5wZGYADgAeAA4AMQA2ADEAMQAuADAANAA1ADUAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MTEuMDQ1NTgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1611.04558}}

@inproceedings{vulic-moens-2013-study,
	Address = {Seattle, Washington, USA},
	Author = {Vuli{\'c}, Ivan and Moens, Marie-Francine},
	Booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2020-06-13 00:59:24 +0200},
	Date-Modified = {2020-06-13 00:59:24 +0200},
	Month = oct,
	Pages = {1613--1624},
	Publisher = {Association for Computational Linguistics},
	Title = {A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)},
	Url = {https://www.aclweb.org/anthology/D13-1168.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9kMTMtMTE2OC5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MZDEzLTExNjgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmQxMy0xMTY4LnBkZgAOABoADABkADEAMwAtADEAMQA2ADgALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9kMTMtMTE2OC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D13-1168}}

@article{Hermann:2013aa,
	Abstract = {Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.},
	Author = {Karl Moritz Hermann and Phil Blunsom},
	Date-Added = {2020-06-13 00:57:25 +0200},
	Date-Modified = {2020-06-13 00:57:25 +0200},
	Eprint = {1312.6173},
	Month = {12},
	Title = {Multilingual Distributed Representations without Word Alignment},
	Url = {https://arxiv.org/pdf/1312.6173.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzEyLjYxNzMucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMTIuNjE3My5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzEyLjYxNzMucGRmAAAOABwADQAxADMAMQAyAC4ANgAxADcAMwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMTIuNjE3My5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1312.6173}}

@article{Kim:2019aa,
	Abstract = {Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pre-trained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pre-training data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pre-trained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer.},
	Author = {Yunsu Kim and Yingbo Gao and Hermann Ney},
	Date-Added = {2020-06-03 18:27:16 +0200},
	Date-Modified = {2020-06-03 18:27:16 +0200},
	Eprint = {1905.05475},
	Month = {05},
	Title = {Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies},
	Url = {https://arxiv.org/pdf/1905.05475.pdf},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xOTA1LjA1NDc1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xOTA1LjA1NDc1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTkwNS4wNTQ3NS5wZGYADgAeAA4AMQA5ADAANQAuADAANQA0ADcANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE5MDUuMDU0NzUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1905.05475}}

@article{Baziotis:2020aa,
	Abstract = {The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM "disagrees" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.},
	Author = {Christos Baziotis and Barry Haddow and Alexandra Birch},
	Date-Added = {2020-06-03 18:26:35 +0200},
	Date-Modified = {2020-06-03 18:26:35 +0200},
	Eprint = {2004.14928},
	Month = {04},
	Title = {Language Model Prior for Low-Resource Neural Machine Translation},
	Url = {https://arxiv.org/pdf/2004.14928.pdf},
	Year = {2020},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8yMDA0LjE0OTI4LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4yMDA0LjE0OTI4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MjAwNC4xNDkyOC5wZGYADgAeAA4AMgAwADAANAAuADEANAA5ADIAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzIwMDQuMTQ5MjgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/2004.14928}}

@article{Mikolov:2013ac,
	Abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
	Author = {Tomas Mikolov and Quoc V. Le and Ilya Sutskever},
	Date-Added = {2020-06-01 16:44:59 +0200},
	Date-Modified = {2020-06-13 00:18:49 +0200},
	Eprint = {1309.4168},
	Month = {09},
	Title = {Exploiting Similarities among Languages for Machine Translation},
	Url = {https://arxiv.org/pdf/1309.4168.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzA5LjQxNjgucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMDkuNDE2OC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzA5LjQxNjgucGRmAAAOABwADQAxADMAMAA5AC4ANAAxADYAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMDkuNDE2OC5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1309.4168}}

@article{levy-etal-2015-improving,
	Abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
	Author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
	Date-Added = {2020-05-29 16:13:23 +0200},
	Date-Modified = {2020-05-29 16:13:23 +0200},
	Doi = {10.1162/tacl_a_00134},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {211--225},
	Title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
	Url = {https://www.aclweb.org/anthology/Q15-1016.pdf},
	Volume = {3},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9xMTUtMTAxNi5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8McTE1LTEwMTYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOnExNS0xMDE2LnBkZgAOABoADABxADEANQAtADEAMAAxADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9xMTUtMTAxNi5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/Q15-1016}}

@article{Ruder:2019aa,
	Abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
	Author = {Sebastian Ruder and Ivan Vuli{\'c} and Anders S{\o}gaard},
	Date-Added = {2020-05-26 15:52:14 +0200},
	Date-Modified = {2020-05-26 15:52:14 +0200},
	Doi = {10.1613/jair.1.11640},
	Eprint = {1706.04902},
	Journal = {JAIR},
	Pages = {569-631},
	Title = {A Survey Of Cross-lingual Word Embedding Models},
	Url = {https://arxiv.org/pdf/1706.04902.pdf},
	Volume = {65},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzA2LjA0OTAyLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzA2LjA0OTAyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcwNi4wNDkwMi5wZGYADgAeAA4AMQA3ADAANgAuADAANAA5ADAAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MDYuMDQ5MDIucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1706.04902},
	Bdsk-Url-2 = {https://doi.org/10.1613/jair.1.11640}}

@article{Qi:2018aa,
	Abstract = {The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases -- providing gains of up to 20 BLEU points in the most favorable setting.},
	Author = {Ye Qi and Devendra Singh Sachan and Matthieu Felix and Sarguna Janani Padmanabhan and Graham Neubig},
	Date-Added = {2020-05-23 23:38:33 +0200},
	Date-Modified = {2020-05-23 23:38:33 +0200},
	Eprint = {1804.06323},
	Month = {04},
	Title = {When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?},
	Url = {https://arxiv.org/pdf/1804.06323.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODA0LjA2MzIzLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODA0LjA2MzIzLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgwNC4wNjMyMy5wZGYADgAeAA4AMQA4ADAANAAuADAANgAzADIAMwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MDQuMDYzMjMucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1804.06323}}

@inproceedings{artetxe-etal-2017-learning,
	Abstract = {Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.},
	Address = {Vancouver, Canada},
	Author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
	Booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Date-Added = {2020-05-23 23:04:49 +0200},
	Date-Modified = {2020-05-23 23:04:49 +0200},
	Doi = {10.18653/v1/P17-1042},
	Month = jul,
	Pages = {451--462},
	Publisher = {Association for Computational Linguistics},
	Title = {Learning bilingual word embeddings with (almost) no bilingual data},
	Url = {https://www.aclweb.org/anthology/P17-1042.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9wMTctMTA0Mi5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8McDE3LTEwNDIucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOnAxNy0xMDQyLnBkZgAOABoADABwADEANwAtADEAMAA0ADIALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9wMTctMTA0Mi5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P17-1042}}

@inproceedings{zou-etal-2013-bilingual,
	Abstract = {We introduce bilingual word embeddings: se- mantic embeddings associated across two lan- guages in the context of neural language mod- els. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to con- strain translational equivalence. The new em- beddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual em- beddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.},
	Address = {Seattle, Washington, USA},
	Author = {Zou, Will Y. and Socher, Richard and Cer, Daniel and Manning, Christopher D.},
	Booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2020-05-23 22:40:38 +0200},
	Date-Modified = {2020-06-13 00:04:12 +0200},
	Month = oct,
	Pages = {1393--1398},
	Publisher = {Association for Computational Linguistics},
	Title = {Bilingual Word Embeddings for Phrase-Based Machine Translation},
	Url = {https://www.aclweb.org/anthology/D13-1141.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9kMTMtMTE0MS5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MZDEzLTExNDEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOmQxMy0xMTQxLnBkZgAOABoADABkADEAMwAtADEAMQA0ADEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9kMTMtMTE0MS5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D13-1141}}

@article{Conneau:2017aa,
	Abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
	Author = {Alexis Conneau and Guillaume Lample and Marc'Aurelio Ranzato and Ludovic Denoyer and Herv{\'e} J{\'e}gou},
	Date-Added = {2020-05-18 12:57:39 +0200},
	Date-Modified = {2020-05-18 12:57:39 +0200},
	Eprint = {1710.04087},
	Month = {10},
	Title = {Word Translation Without Parallel Data},
	Url = {https://arxiv.org/pdf/1710.04087.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzEwLjA0MDg3LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzEwLjA0MDg3LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcxMC4wNDA4Ny5wZGYADgAeAA4AMQA3ADEAMAAuADAANAAwADgANwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MTAuMDQwODcucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1710.04087}}

@article{Upadhyay:2016aa,
	Abstract = {Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.},
	Author = {Shyam Upadhyay and Manaal Faruqui and Chris Dyer and Dan Roth},
	Date-Added = {2020-05-18 11:57:35 +0200},
	Date-Modified = {2020-05-18 11:57:35 +0200},
	Eprint = {1604.00425},
	Month = {04},
	Title = {Cross-lingual Models of Word Embeddings: An Empirical Comparison},
	Url = {https://arxiv.org/pdf/1604.00425.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA0LjAwNDI1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA0LjAwNDI1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNC4wMDQyNS5wZGYADgAeAA4AMQA2ADAANAAuADAAMAA0ADIANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDQuMDA0MjUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1604.00425}}

@article{bengio2003neural,
	Author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
	Date-Added = {2020-05-18 11:43:11 +0200},
	Date-Modified = {2020-07-09 23:57:27 +0200},
	Issn = {1532-4435},
	Issue_Date = {3/1/2003},
	Journal = {J. Mach. Learn. Res.},
	Month = mar,
	Number = {null},
	Numpages = {19},
	Pages = {1137--1155},
	Publisher = {JMLR.org},
	Title = {A Neural Probabilistic Language Model},
	Volume = {3},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxArLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy85NDQ5MTkuOTQ0OTY2LnBkZk8RAWAAAAAAAWAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xE5NDQ5MTkuOTQ0OTY2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAMS86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6OTQ0OTE5Ljk0NDk2Ni5wZGYAAA4AJAARADkANAA0ADkAMQA5AC4AOQA0ADQAOQA2ADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAC9Vc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy85NDQ5MTkuOTQ0OTY2LnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQAUgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAG2},
	Bdsk-Url-1 = {https://dl.acm.org/doi/10.5555/944919.944966}}

@inproceedings{klementiev-etal-2012-inducing,
	Abstract = {Distributed representations of words have proven extremely useful in numerous natural lan- guage processing tasks. Their appeal is that they can help alleviate data sparsity problems common to supervised learning. Methods for inducing these representations require only unlabeled language data, which are plentiful for many natural languages. In this work, we induce distributed representations for a pair of languages jointly. We treat it as a multitask learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. These representations can be used for a number of crosslingual learning tasks, where a learner can be trained on annotations present in one language and applied to test data in another. We show that our representations are informative by using them for crosslingual document classification, where classifiers trained on these representations substantially outperform strong baselines (e.g. machine translation) when applied to a new language.},
	Address = {Mumbai, India},
	Author = {Klementiev, Alexandre and Titov, Ivan and Bhattarai, Binod},
	Booktitle = {Proceedings of {COLING} 2012},
	Date-Added = {2020-05-18 11:24:23 +0200},
	Date-Modified = {2020-05-18 11:31:20 +0200},
	Month = dec,
	Pages = {1459--1474},
	Publisher = {The COLING 2012 Organizing Committee},
	Title = {Inducing Crosslingual Distributed Representations of Words},
	Url = {https://www.aclweb.org/anthology/C12-1089.pdf},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9DMTItMTA4OS5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MQzEyLTEwODkucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOkMxMi0xMDg5LnBkZgAOABoADABDADEAMgAtADEAMAA4ADkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9DMTItMTA4OS5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/C12-1089}}

@article{Ammar:2016aa,
	Abstract = {We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC-CCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.},
	Author = {Waleed Ammar and George Mulcaire and Yulia Tsvetkov and Guillaume Lample and Chris Dyer and Noah A. Smith},
	Date-Added = {2020-05-18 11:14:22 +0200},
	Date-Modified = {2020-05-18 11:14:22 +0200},
	Eprint = {1602.01925},
	Month = {02},
	Title = {Massively Multilingual Word Embeddings},
	Url = {https://arxiv.org/pdf/1602.01925.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjAyLjAxOTI1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjAyLjAxOTI1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwMi4wMTkyNS5wZGYADgAeAA4AMQA2ADAAMgAuADAAMQA5ADIANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDIuMDE5MjUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1602.01925}}

@article{Bojanowski:2016aa,
	Abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	Author = {Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
	Date-Added = {2020-05-16 12:44:20 +0200},
	Date-Modified = {2020-05-16 12:44:20 +0200},
	Eprint = {1607.04606},
	Month = {07},
	Title = {Enriching Word Vectors with Subword Information},
	Url = {https://arxiv.org/pdf/1607.04606.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA3LjA0NjA2LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA3LjA0NjA2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNy4wNDYwNi5wZGYADgAeAA4AMQA2ADAANwAuADAANAA2ADAANgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDcuMDQ2MDYucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1607.04606}}

@article{Joulin:2018aa,
	Abstract = {Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a least-square regression problem to learn a rotation aligning a small bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.},
	Author = {Armand Joulin and Piotr Bojanowski and Tomas Mikolov and Herve Jegou and Edouard Grave},
	Date-Added = {2020-05-16 12:44:04 +0200},
	Date-Modified = {2020-05-16 12:44:04 +0200},
	Eprint = {1804.07745},
	Month = {04},
	Title = {Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion},
	Url = {https://arxiv.org/pdf/1804.07745.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODA0LjA3NzQ1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODA0LjA3NzQ1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgwNC4wNzc0NS5wZGYADgAeAA4AMQA4ADAANAAuADAANwA3ADQANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MDQuMDc3NDUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1804.07745}}

@inproceedings{ri2019multilingual,
	Author = {Ryokan Ri and Yoshimasa Tsuruoka},
	Booktitle = {:},
	Date-Added = {2020-05-16 11:38:32 +0200},
	Date-Modified = {2020-05-18 11:47:50 +0200},
	Title = {What do Multilingual Neural Machine Translation Models learn about Typology?},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAfLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy82LnBkZk8RATAAAAAAATAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////wU2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAJS86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6Ni5wZGYAAA4ADAAFADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACNVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy82LnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQARgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAF6}}

@inproceedings{Basirat1347044,
	Author = {Basirat, Ali and de Lhoneux, Miryam and Kulmizev, Artur and Kurfal, Murathan and Nivre, Joakim and {\"O}stling, Robert},
	Booktitle = {:},
	Date-Added = {2020-05-16 10:41:34 +0200},
	Date-Modified = {2020-05-16 10:41:34 +0200},
	Institution = {Department of Linguistics, Stockholm University},
	Title = {Polyglot Parsing for One Thousand and One Languages (And Then Some)},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxApLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9GVUxMVEVYVDAxMC5wZGZPEQFYAAAAAAFYAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8PRlVMTFRFWFQwMTAucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAC8vOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOkZVTExURVhUMDEwLnBkZgAADgAgAA8ARgBVAEwATABUAEUAWABUADAAMQAwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgAtVXNlcnMvc2hpZmVpL0RvY3VtZW50cy9QYXBlcnMvRlVMTFRFWFQwMTAucGRmAAATAAEvAAAVAAIADf//AAAACAANABoAJABQAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAaw=},
	Bdsk-Url-1 = {http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-392156}}

@article{Al-Rfou:2013aa,
	Abstract = {Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.},
	Author = {Rami Al-Rfou and Bryan Perozzi and Steven Skiena},
	Date-Added = {2020-05-16 10:41:08 +0200},
	Date-Modified = {2020-05-16 10:41:08 +0200},
	Eprint = {1307.1662},
	Month = {07},
	Title = {Polyglot: Distributed Word Representations for Multilingual NLP},
	Url = {https://arxiv.org/pdf/1307.1662.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzA3LjE2NjIucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMDcuMTY2Mi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzA3LjE2NjIucGRmAAAOABwADQAxADMAMAA3AC4AMQA2ADYAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMDcuMTY2Mi5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1307.1662}}

@mastersthesis{Dyer1365879,
	Abstract = {Cross-lingual word embeddings are an increasingly important reseource in cross-lingual methods for NLP, particularly for their role in transfer learning and unsupervised machine translation, purportedly opening up the opportunity for NLP applications for low-resource languages.  However, most research in this area implicitly expects the availablility of vast monolingual corpora for training embeddings, a scenario which is not realistic for many of the world's languages.  Moreover, much of the reporting of the performance of cross-lingual word embeddings is based on a fairly narrow set of mostly European language pairs.  Our study examines the performance of cross-lingual alignment across a more diverse set of language pairs; controls for the effect of the corpus size on which the monolingual embedding spaces are trained; and studies the impact of spectral graph properties of the embedding spsace on alignment.  Through our experiments on a more diverse set of language pairs, we find that performance in bilingual lexicon induction is generally poor in heterogeneous pairs, and that even using a gold or heuristically derived dictionary has little impact on the performance on these pairs of languages.  We also find that the performance for these languages only increases slowly with corpus size.  Finally, we find a moderate correlation between the isospectral difference of the source and target embeddings and the performance of bilingual lexicon induction.  We infer that methods other than cross-lingual alignment may be more appropriate in the case of both low resource languages and heterogeneous language pairs. },
	Author = {Dyer, Andrew},
	Date-Added = {2020-05-16 10:25:31 +0200},
	Date-Modified = {2020-05-16 10:25:31 +0200},
	Institution = {Uppsala University, Department of Linguistics and Philology},
	Keywords = {word embeddings, cross-lingual, multilingual, low-resource, corpus size, Vecmap, FastText, alignment, orthogonal, eigenvalues, Laplacian, isospectral, isomorphic, bilingual lexicon induction},
	Pages = {53},
	School = {Uppsala University, Department of Linguistics and Philology},
	Title = {Low Supervision, Low Corpus size, Low Similarity! Challenges in cross-lingual alignment of word embeddings : An exploration of the limitations of cross-lingual word embedding alignment in truly low resource scenarios},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9GVUxMVEVYVDAxLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w5GVUxMVEVYVDAxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6RlVMTFRFWFQwMS5wZGYADgAeAA4ARgBVAEwATABUAEUAWABUADAAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzL0ZVTExURVhUMDEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-395946}}

@article{Mikolov:2013ab,
	Abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	Author = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
	Date-Added = {2020-05-16 10:21:50 +0200},
	Date-Modified = {2020-06-13 00:04:14 +0200},
	Eprint = {1310.4546},
	Month = {10},
	Title = {Distributed Representations of Words and Phrases and their Compositionality},
	Url = {https://arxiv.org/pdf/1310.4546.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzEwLjQ1NDYucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMTAuNDU0Ni5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzEwLjQ1NDYucGRmAAAOABwADQAxADMAMQAwAC4ANAA1ADQANgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMTAuNDU0Ni5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1310.4546}}

@article{Mikolov:2013aa,
	Abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	Author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	Date-Added = {2020-05-16 10:21:12 +0200},
	Date-Modified = {2020-06-13 00:04:08 +0200},
	Eprint = {1301.3781},
	Month = {01},
	Title = {Efficient Estimation of Word Representations in Vector Space},
	Url = {https://arxiv.org/pdf/1301.3781.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzAxLjM3ODEucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMDEuMzc4MS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzAxLjM3ODEucGRmAAAOABwADQAxADMAMAAxAC4AMwA3ADgAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMDEuMzc4MS5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1301.3781}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>Language Embeddings</string>
		<key>keys</key>
		<string>littell-etal-2017-uriel,malaviya-etal-2017-learning,Bjerva_2019</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Machine Translation</string>
		<key>keys</key>
		<string>Johnson:2016aa,Conneau:2017aa,Kim:2019aa,Baziotis:2020aa,Ha:2017aa,Lakew:2018aa,Arivazhagan:2019aa,Lakew:2019aa,Ha:2016aa,Lakew:2018ab,Firat:2016aa,Blackwood:2018aa,Vaswani:2017aa</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Word Embeddings</string>
		<key>keys</key>
		<string>bengio2003neural,klementiev-etal-2012-inducing,Mikolov:2013aa,Al-Rfou:2013aa,vulic-moens-2013-study,zou-etal-2013-bilingual,Mikolov:2013ab,Hermann:2013aa,levy-etal-2015-improving,Ammar:2016aa,Upadhyay:2016aa,Bojanowski:2016aa,Smith:2017aa,artetxe-etal-2017-learning,Joulin:2018aa,Qi:2018aa,Ruder:2019aa,Dyer1365879,Artetxe:2017aa,neishi-etal-2017-bag,inproceedings,Heinzerling:2019aa,glorot2010understanding,Mikolov:2013ac</string>
	</dict>
</array>
</plist>
}}
