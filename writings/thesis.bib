%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Shifei Chen at 2020-05-18 12:57:53 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{Conneau:2017aa,
	Abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
	Author = {Alexis Conneau and Guillaume Lample and Marc'Aurelio Ranzato and Ludovic Denoyer and Herv{\'e} J{\'e}gou},
	Date-Added = {2020-05-18 12:57:39 +0200},
	Date-Modified = {2020-05-18 12:57:39 +0200},
	Eprint = {1710.04087},
	Month = {10},
	Title = {Word Translation Without Parallel Data},
	Url = {https://arxiv.org/pdf/1710.04087.pdf},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNzEwLjA0MDg3LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNzEwLjA0MDg3LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTcxMC4wNDA4Ny5wZGYADgAeAA4AMQA3ADEAMAAuADAANAAwADgANwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE3MTAuMDQwODcucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1710.04087}}

@article{Upadhyay:2016aa,
	Abstract = {Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.},
	Author = {Shyam Upadhyay and Manaal Faruqui and Chris Dyer and Dan Roth},
	Date-Added = {2020-05-18 11:57:35 +0200},
	Date-Modified = {2020-05-18 11:57:35 +0200},
	Eprint = {1604.00425},
	Month = {04},
	Title = {Cross-lingual Models of Word Embeddings: An Empirical Comparison},
	Url = {https://arxiv.org/pdf/1604.00425.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA0LjAwNDI1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA0LjAwNDI1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNC4wMDQyNS5wZGYADgAeAA4AMQA2ADAANAAuADAAMAA0ADIANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDQuMDA0MjUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1604.00425}}

@article{bengio2003neural,
	Author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
	Date-Added = {2020-05-18 11:43:11 +0200},
	Date-Modified = {2020-05-18 11:47:27 +0200},
	Issn = {1532-4435},
	Issue_Date = {3/1/2003},
	Journal = {J. Mach. Learn. Res.},
	Month = mar,
	Number = {null},
	Numpages = {19},
	Pages = {1137--1155},
	Publisher = {JMLR.org},
	Title = {A Neural Probabilistic Language Model},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {https://dl.acm.org/doi/10.5555/944919.944966},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxArLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy85NDQ5MTkuOTQ0OTY2LnBkZk8RAWAAAAAAAWAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xE5NDQ5MTkuOTQ0OTY2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAMS86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6OTQ0OTE5Ljk0NDk2Ni5wZGYAAA4AJAARADkANAA0ADkAMQA5AC4AOQA0ADQAOQA2ADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAC9Vc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy85NDQ5MTkuOTQ0OTY2LnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQAUgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAG2}}

@inproceedings{klementiev-etal-2012-inducing,
	Abstract = {Distributed representations of words have proven extremely useful in numerous natural lan- guage processing tasks. Their appeal is that they can help alleviate data sparsity problems common to supervised learning. Methods for inducing these representations require only unlabeled language data, which are plentiful for many natural languages. In this work, we induce distributed representations for a pair of languages jointly. We treat it as a multitask learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. These representations can be used for a number of crosslingual learning tasks, where a learner can be trained on annotations present in one language and applied to test data in another. We show that our representations are informative by using them for crosslingual document classification, where classifiers trained on these representations substantially outperform strong baselines (e.g. machine translation) when applied to a new language.},
	Address = {Mumbai, India},
	Author = {Klementiev, Alexandre and Titov, Ivan and Bhattarai, Binod},
	Booktitle = {Proceedings of {COLING} 2012},
	Date-Added = {2020-05-18 11:24:23 +0200},
	Date-Modified = {2020-05-18 11:31:20 +0200},
	Month = dec,
	Pages = {1459--1474},
	Publisher = {The COLING 2012 Organizing Committee},
	Title = {Inducing Crosslingual Distributed Representations of Words},
	Url = {https://www.aclweb.org/anthology/C12-1089.pdf},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9DMTItMTA4OS5wZGZPEQFKAAAAAAFKAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8MQzEyLTEwODkucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACACwvOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOkMxMi0xMDg5LnBkZgAOABoADABDADEAMgAtADEAMAA4ADkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACpVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy9DMTItMTA4OS5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQATQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAGb},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/C12-1089}}

@article{Ammar:2016aa,
	Abstract = {We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC-CCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.},
	Author = {Waleed Ammar and George Mulcaire and Yulia Tsvetkov and Guillaume Lample and Chris Dyer and Noah A. Smith},
	Date-Added = {2020-05-18 11:14:22 +0200},
	Date-Modified = {2020-05-18 11:14:22 +0200},
	Eprint = {1602.01925},
	Month = {02},
	Title = {Massively Multilingual Word Embeddings},
	Url = {https://arxiv.org/pdf/1602.01925.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjAyLjAxOTI1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjAyLjAxOTI1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwMi4wMTkyNS5wZGYADgAeAA4AMQA2ADAAMgAuADAAMQA5ADIANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDIuMDE5MjUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1602.01925}}

@article{Bojanowski:2016aa,
	Abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	Author = {Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
	Date-Added = {2020-05-16 12:44:20 +0200},
	Date-Modified = {2020-05-16 12:44:20 +0200},
	Eprint = {1607.04606},
	Month = {07},
	Title = {Enriching Word Vectors with Subword Information},
	Url = {https://arxiv.org/pdf/1607.04606.pdf},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xNjA3LjA0NjA2LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xNjA3LjA0NjA2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTYwNy4wNDYwNi5wZGYADgAeAA4AMQA2ADAANwAuADAANAA2ADAANgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE2MDcuMDQ2MDYucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1607.04606}}

@article{Joulin:2018aa,
	Abstract = {Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a least-square regression problem to learn a rotation aligning a small bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.},
	Author = {Armand Joulin and Piotr Bojanowski and Tomas Mikolov and Herve Jegou and Edouard Grave},
	Date-Added = {2020-05-16 12:44:04 +0200},
	Date-Modified = {2020-05-16 12:44:04 +0200},
	Eprint = {1804.07745},
	Month = {04},
	Title = {Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion},
	Url = {https://arxiv.org/pdf/1804.07745.pdf},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xODA0LjA3NzQ1LnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w4xODA0LjA3NzQ1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6MTgwNC4wNzc0NS5wZGYADgAeAA4AMQA4ADAANAAuADAANwA3ADQANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzE4MDQuMDc3NDUucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1804.07745}}

@inproceedings{ri2019multilingual,
	Author = {Ryokan Ri and Yoshimasa Tsuruoka},
	Booktitle = {:},
	Date-Added = {2020-05-16 11:38:32 +0200},
	Date-Modified = {2020-05-18 11:47:50 +0200},
	Title = {What do Multilingual Neural Machine Translation Models learn about Typology?},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAfLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy82LnBkZk8RATAAAAAAATAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////wU2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAJS86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6Ni5wZGYAAA4ADAAFADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASACNVc2Vycy9zaGlmZWkvRG9jdW1lbnRzL1BhcGVycy82LnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQARgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAF6}}

@inproceedings{Basirat1347044,
	Author = {Basirat, Ali and de Lhoneux, Miryam and Kulmizev, Artur and Kurfal, Murathan and Nivre, Joakim and {\"O}stling, Robert},
	Booktitle = {:},
	Date-Added = {2020-05-16 10:41:34 +0200},
	Date-Modified = {2020-05-16 10:41:34 +0200},
	Institution = {Department of Linguistics, Stockholm University},
	Title = {Polyglot Parsing for One Thousand and One Languages (And Then Some)},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxApLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9GVUxMVEVYVDAxMC5wZGZPEQFYAAAAAAFYAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8PRlVMTFRFWFQwMTAucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAC8vOlVzZXJzOnNoaWZlaTpEb2N1bWVudHM6UGFwZXJzOkZVTExURVhUMDEwLnBkZgAADgAgAA8ARgBVAEwATABUAEUAWABUADAAMQAwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgAtVXNlcnMvc2hpZmVpL0RvY3VtZW50cy9QYXBlcnMvRlVMTFRFWFQwMTAucGRmAAATAAEvAAAVAAIADf//AAAACAANABoAJABQAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAaw=},
	Bdsk-Url-1 = {http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-392156}}

@article{Al-Rfou:2013aa,
	Abstract = {Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.},
	Author = {Rami Al-Rfou and Bryan Perozzi and Steven Skiena},
	Date-Added = {2020-05-16 10:41:08 +0200},
	Date-Modified = {2020-05-16 10:41:08 +0200},
	Eprint = {1307.1662},
	Month = {07},
	Title = {Polyglot: Distributed Word Representations for Multilingual NLP},
	Url = {https://arxiv.org/pdf/1307.1662.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzA3LjE2NjIucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMDcuMTY2Mi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzA3LjE2NjIucGRmAAAOABwADQAxADMAMAA3AC4AMQA2ADYAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMDcuMTY2Mi5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1307.1662}}

@mastersthesis{Dyer1365879,
	Abstract = {Cross-lingual word embeddings are an increasingly important reseource in cross-lingual methods for NLP, particularly for their role in transfer learning and unsupervised machine translation, purportedly opening up the opportunity for NLP applications for low-resource languages.  However, most research in this area implicitly expects the availablility of vast monolingual corpora for training embeddings, a scenario which is not realistic for many of the world's languages.  Moreover, much of the reporting of the performance of cross-lingual word embeddings is based on a fairly narrow set of mostly European language pairs.  Our study examines the performance of cross-lingual alignment across a more diverse set of language pairs; controls for the effect of the corpus size on which the monolingual embedding spaces are trained; and studies the impact of spectral graph properties of the embedding spsace on alignment.  Through our experiments on a more diverse set of language pairs, we find that performance in bilingual lexicon induction is generally poor in heterogeneous pairs, and that even using a gold or heuristically derived dictionary has little impact on the performance on these pairs of languages.  We also find that the performance for these languages only increases slowly with corpus size.  Finally, we find a moderate correlation between the isospectral difference of the source and target embeddings and the performance of bilingual lexicon induction.  We infer that methods other than cross-lingual alignment may be more appropriate in the case of both low resource languages and heterogeneous language pairs. },
	Author = {Dyer, Andrew},
	Date-Added = {2020-05-16 10:25:31 +0200},
	Date-Modified = {2020-05-16 10:25:31 +0200},
	Institution = {Uppsala University, Department of Linguistics and Philology},
	Keywords = {word embeddings, cross-lingual, multilingual, low-resource, corpus size, Vecmap, FastText, alignment, orthogonal, eigenvalues, Laplacian, isospectral, isomorphic, bilingual lexicon induction},
	Pages = {53},
	School = {Uppsala University, Department of Linguistics and Philology},
	Title = {Low Supervision, Low Corpus size, Low Similarity! Challenges in cross-lingual alignment of word embeddings : An exploration of the limitations of cross-lingual word embedding alignment in truly low resource scenarios},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAoLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy9GVUxMVEVYVDAxLnBkZk8RAVIAAAAAAVIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w5GVUxMVEVYVDAxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAwAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIALi86VXNlcnM6c2hpZmVpOkRvY3VtZW50czpQYXBlcnM6RlVMTFRFWFQwMS5wZGYADgAeAA4ARgBVAEwATABUAEUAWABUADAAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIALFVzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzL0ZVTExURVhUMDEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABpQ==},
	Bdsk-Url-1 = {http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-395946}}

@article{Mikolov:2013ab,
	Abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	Author = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
	Date-Added = {2020-05-16 10:21:50 +0200},
	Date-Modified = {2020-05-16 10:21:50 +0200},
	Eprint = {1310.4546},
	Month = {10},
	Title = {Distributed Representations of Words and Phrases and their Compositionality},
	Url = {https://arxiv.org/pdf/1310.4546.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzEwLjQ1NDYucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMTAuNDU0Ni5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzEwLjQ1NDYucGRmAAAOABwADQAxADMAMQAwAC4ANAA1ADQANgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMTAuNDU0Ni5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1310.4546}}

@article{Mikolov:2013aa,
	Abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	Author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	Date-Added = {2020-05-16 10:21:12 +0200},
	Date-Modified = {2020-05-16 10:21:12 +0200},
	Eprint = {1301.3781},
	Month = {01},
	Title = {Efficient Estimation of Word Representations in Vector Space},
	Url = {https://arxiv.org/pdf/1301.3781.pdf},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAnLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycy8xMzAxLjM3ODEucGRmTxEBUAAAAAABUAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DTEzMDEuMzc4MS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAwADAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgAtLzpVc2VyczpzaGlmZWk6RG9jdW1lbnRzOlBhcGVyczoxMzAxLjM3ODEucGRmAAAOABwADQAxADMAMAAxAC4AMwA3ADgAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAK1VzZXJzL3NoaWZlaS9Eb2N1bWVudHMvUGFwZXJzLzEzMDEuMzc4MS5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAE4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABog==},
	Bdsk-Url-1 = {https://arxiv.org/abs/1301.3781}}
